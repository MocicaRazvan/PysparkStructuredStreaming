{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark Structured Streaming\n",
    "\n",
    "---\n",
    "\n",
    "In cadrul acestui referat voi prezenta Structured Streaming, care este un mod de a scrie aplicatii de streaming scalabile si fault-tolerante folosind Apache Spark SQL, care permite scrierea de cod similar cu cel folosit pentru batch processing si care beneficiaza de aceleasi optimizari ca acesta. Spark Structured Streaming poate fi folosit in oricare dintre limbajele suportate de Spark, cum ar fi Scala, Java, Python sau R. Structured Streaming este construit pe baza API-ului DataFrame si Dataset, care ofera o abordare mai simpla si mai puternica pentru a lucra cu datele de streaming. Strucred streaming este un sistem de procesare a datelor de streaming care ofera garantii de livrare a datelor (end-to-end-exactly-once), toleranta la erori si scalabilitate.\n",
    "In cadrul referatului voi prezenta cum putem citi datele de streaming din surse precum socket-uri, fisiere sau Kafka, cum putem scrie datele in sink-uri precum console, fisiere sau Kafka, cum putem face operatii de windowing si cum putem folosi shuffle partition pentru a imbunatati performanta aplicatiei noastre. De asemnea, voi prezenta putin o parte de optimizare pentru a intelege mai bine cum functioneaza Spark in spate. Limbajul folosit va fi Python, iar mediul de lucru va fi Jupyter Notebook.\n",
    "\n",
    "---\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "1. [Setting Up The Environment For Windows (Optional)](#crearea-unui-enviroment-de-lucru-local-pentru-windows)\n",
    "1. [Introduction](#introduction)\n",
    "1. [Reading Data](#reading-data)\n",
    "   1. [Reading From Sockets](#reading-from-sockets)\n",
    "   1. [Spark UI](#spark-ui)\n",
    "   1. [Reading From Files](#reading-from-files)\n",
    "1. [Writing Data And Windowing](#writing-data-and-windowing)\n",
    "   1. [Complete Mode](#complete-mode)\n",
    "   1. [Update Mode](#update-mode)\n",
    "   1. [Window Operations And Append Mode](#window-operations-and-append-mode)\n",
    "   1. [Session Window](#session-window)\n",
    "   1. [Writing To Files](#writing-to-files)\n",
    "   1. [Triggers](#triggers)\n",
    "1. [Shuffle Partition](#shuffle-partition)\n",
    "1. [Kafka](#kafka)\n",
    "   1. [Overview](#overview)\n",
    "   1. [Local Setup](#local-setup)\n",
    "   1. [Hello World Kafka](#hello-world-kafka)\n",
    "   1. [Kafka With Spark](#kafka-with-spark)\n",
    "      1. [Reading From Kafka](#reading-from-kafka)\n",
    "      1. [Writing To Kafka](#writing-to-kafka)\n",
    "1. [Writing To Multiple Sinks](#writing-to-multiple-sinks)\n",
    "   1. [For Each Batch](#foreachbatch)\n",
    "   1. [Setup The Environment](#setup-the-environment)\n",
    "1. [Bibliografie](#bibliografie)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up The Environment For Windows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inainte sa incep prezentarea continutului efectiv, as dori sa arat un mod de a crea un mediu de lucru local pentru Windows cu ajutorul unui container Docker. Acest lucru este necesar deoarece Spark nu este disponibil in mod nativ pe Windows, iar celelalte metode de instalare sunt destul de complicate si pot cauza probleme (mai ales in lucurul cu fisierele).\n",
    "\n",
    "Inainte de toate, trebuie sa aveti instalat Docker Desktop pe calculator. Daca nu il aveti, il puteti descarca de [aici](https://www.docker.com/products/docker-desktop). Eu personal voi folosi editorul de text [Visual Studio Code](https://code.visualstudio.com/), dar puteti folosi orice editor doriti sau Jupyter Notebook.\n",
    "\n",
    "Dupa ce s-a instalat Docker Desktop, creati un fisier numit `docker-compose.yml` in care sa scrieti urmatorul cod:\n",
    "\n",
    "```yaml\n",
    "services:\n",
    "  bd-jupyter-notebook:\n",
    "    image: jupyter/pyspark-notebook:latest\n",
    "    user: root\n",
    "    container_name: bd-jupyter-notebook-lab\n",
    "    ports:\n",
    "      - 8888:8888\n",
    "      - 4040:4040\n",
    "    environment:\n",
    "      JUPYTER_PORT: 8888\n",
    "      SPARK_UI_PORT: 4040\n",
    "      GRANT_SUDO: yes\n",
    "    volumes:\n",
    "      - <cale catre un fisier dorit>:/home/jovyan:rw\n",
    "      - streaming_data:/data:rw\n",
    "\n",
    "  bd-zookeeper:\n",
    "    image: confluentinc/cp-zookeeper:latest\n",
    "    container_name: bd-zookeeper\n",
    "    ports:\n",
    "      - 2181:2181\n",
    "    environment:\n",
    "      ZOOKEEPER_CLIENT_PORT: 2181\n",
    "      ZOOKEEPER_TICK_TIME: 2000\n",
    "\n",
    "  bd-kafka:\n",
    "    image: confluentinc/cp-kafka:latest\n",
    "    container_name: bd-kafka\n",
    "    depends_on:\n",
    "      - bd-zookeeper\n",
    "    ports:\n",
    "      - 9092:9092\n",
    "    volumes:\n",
    "      - streaming_data:/data:rw\n",
    "    environment:\n",
    "      KAFKA_BROKER_ID: 1\n",
    "      KAFKA_ZOOKEEPER_CONNECT: bd-zookeeper:2181\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://bd-kafka:29092,PLAINTEXT_HOST://127.0.0.1:9092\n",
    "      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n",
    "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "\n",
    "  bd-postgres:\n",
    "    image: postgres:latest\n",
    "    container_name: bd-postgres\n",
    "    environment:\n",
    "      POSTGRES_DB: mydb\n",
    "      POSTGRES_USER: myuser\n",
    "      POSTGRES_PASSWORD: mypassword\n",
    "    ports:\n",
    "      - 5432:5432\n",
    "    volumes:\n",
    "      - posgres_bd:/var/lib/postgresql/data\n",
    "  bd-pgadmin4:\n",
    "    image: dpage/pgadmin4\n",
    "    container_name: bd-pgadmin4\n",
    "    environment:\n",
    "      PGADMIN_DEFAULT_EMAIL: admin@admin.com\n",
    "      PGADMIN_DEFAULT_PASSWORD: admin\n",
    "    ports:\n",
    "      - 5050:80\n",
    "    depends_on:\n",
    "      - bd-postgres\n",
    "\n",
    "volumes:\n",
    "  streaming_data:\n",
    "  posgres_bd:\n",
    "```\n",
    "\n",
    "In acest fisier, am definit patru servicii: un container cu Jupyter Notebook impreuna cu PySpark, un container cu Zookeeper, un container cu Kafka, care depinde de Zookeeper si doua containere pentru baza de date Postgres si PgAdmin4. Linia care contine `<cale catre un fisier dorit>:/home/jovyan:rw` este optionala, dar daca doriti sa aveti un loc unde sa salvati fisierele, in afara de cel unde Docker face acest lucru by defualt, puteti adauga calea catre un folder dorit.\n",
    "\n",
    "Dupa ce ati creat fisierul, deschideti un terminal in folderul unde se afla acesta si rulati comanda `docker-compose up`. Pentru a conecta VS Code cu containerul, deschideti VS Code si instalati extensia pentru [Jupyter](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter). Dupa ce ati instalat extensia, creati un fisier Jupyter Notebook si in partea drepta sus a editorului apasati `Select Kernel`. Dupa aceea, selectati `Existing Jupyter Environment` si introduceti `http://localhost:8888/tree` in campul care apare. Dupa ce ati facut acest pas va va aprea un prompt pentru parola. Pentru a afla parola mergeti in logurile containerului `bd-jupyter-notebook-lab`, unde ar trebuie sa gasiti:\n",
    "\n",
    "![Setup Logs Image](./images/setup/logsSetup.png)\n",
    "\n",
    "Copiati tokenul, ie valoarea lui `?token=`, si introduceti-l in prompt. Dupa ce ati facut acest pas, ar trebui ca totul sa fie gata.\n",
    "\n",
    "Pentru mai multe detalii pentru conectarea unui Jupyter Notebook cu un container, puteti accesa [acest link](https://medium.com/@FredAsDev/connect-vs-code-jupyter-notebook-to-a-jupyter-container-a63293f29325).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structured Streaming este un mod de a scrie aplicatii de streaming scalabile si fault-tolerante folosind Apache Spark SQL, care permite scrierea de cod similar cu cel folosit pentru batch processing si care beneficiaza de aceleasi optimizari ca acesta. Componenta de streaming poate fi folosita in oricare dintre limbajele suportate de Spark, cum ar fi Scala, Java, Python sau R. Structured Streaming este construit pe baza API-ului DataFrame si Dataset, care ofera o abordare mai simpla si mai puternica pentru a lucra cu datele de streaming. Strucred streaming este un sistem de procesare a datelor de streaming care ofera garantii de livrare a datelor (end-to-end-exactly-once), toleranta la erori si scalabilitate.\n",
    "\n",
    "Intern Structured Streaming este construit pe baza conceptului de `micro-batch` processing, unde datele de streaming sunt impartite in micro-batch-uri ('bucati mici'), care sunt procesate folosind API-ul DataFrame si Dataset si care reusesc sa aiba o latenta de pana la 100 milisecunde si o garantie a rezistenti la erori si a livrarii exacte a datelor.\n",
    "In Spark 2.3, s-a introdus suportul pentru `continuous processing`, care permite procesarea continua a datelor de streaming, fara a fi nevoie de micro-batch-uri, insa acesta nu este inca valabil in varianta finala, fiind inca in stadiu experimental si are si anumite dezavantaje:\n",
    "\n",
    "- Lanseaza multiple taskuri lungi (long-running tasks) care citesc in mod continuu datele, le proceseaza si le scriu in sink. Numarul de taskuri este determinat de numarul de partitii ale datelor care pot fi citite in parallel. Asadar, ininte de a folosi aceasta metoda, trebuie sa va asigurati ca aveti suficiente resurse pentru a rula aceste taskuri in paralel. De exemplu, daca aveti un stream de date care are 100 de partitii, atunci veti avea 100 de taskuri care ruleaza in paralel.\n",
    "- Nu sunt disponibile sisteme de garantare a livrarii exacte a datelor si a rezistentei la erori, asa ca trebuie sa va asigurati ca datele sunt scrise in mod atomic in sink-ul dorit.\n",
    "- Nu suporta agregarile de date\n",
    "\n",
    "In Spark Structured Streaming, streamul de intrare este tratat ca un tabel indefinit care poate fi interogat folosind SparkSQL. Fieacre micro-batch este tratat ca un now 'chunk' de linii in acest tabel, iar motorul SparkSQL genereaza interogari pentru acestea ca si pentru liniile statice. Tabelul rezultat este updatat in mod continuu.\n",
    "\n",
    "<div style=\"background-color:white;\">\n",
    "    <img src=\"https://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png\" alt=\"Structured Streaming\">\n",
    "</div>\n",
    "\n",
    "Important de retinut este ca Structured Streaming nu materializeaza intregul tabel, el citeste cele mai recente date disponibile din sursa, le proceseaza, apoi actualizeaza rezultatul si sterge datele sursa vechi. Tine minte doar un numar minimal de informatii necesare pentru a procesa incremental noile date.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading From Sockets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un mod simplu de a crea un stream de date este de a citi datele de la un socket. Pentru a face acest lucru, putem folosi metoda `readStream` a obiectului `SparkSession` si metoda `format` pentru a specifica formatul in care sunt datele, in cazul nostru `socket`, si metoda `option` pentru a specifica hostul si portul de la care se citesc datele. In final, putem apela metoda `load` pentru a crea un DataFrame de streaming.\n",
    "\n",
    "In continuare, inainte de a prezenta modurile de scriere vom afisa in consola folosind metoda `writeStream` a obiectului DataFrame cu outputMode-ul `append`. Acesta va fi vizibil in logurile containerului `bd-jupyter-notebook-lab`. Pentru a inchide stream-ul, vom opri celula de Jupyter Notebook.\n",
    "\n",
    "```python\n",
    "writeStream.format(\"console\").outputMode(\"append\").start().awaitTermination()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:18:36.927983Z",
     "start_time": "2024-05-07T13:18:31.199350Z"
    }
   },
   "source": [
    "# crearea sesiunii Spark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Reading from Sockets\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f7dce4ab390>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10021ba28c44:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Reading from Sockets</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pentru a crea socketul, ne vom conecta la containerul `bd-jupyter-notebook-lab`, deschizand un terminal si ruland comanda\n",
    "\n",
    "```bash\n",
    "docker exec -it bd-jupyter-notebook-lab bash\n",
    "```\n",
    "\n",
    "Dupa ce ne-am conectat la container, rulam comanda pentru instalarea [Netcat](https://en.wikipedia.org/wiki/Netcat)\n",
    "\n",
    "```bash\n",
    "sudo apt-get update && sudo apt-get install -y netcat\n",
    "```\n",
    "\n",
    "Ca sa deschidem un socket pentru portul 9999, rulam comanda\n",
    "\n",
    "```bash\n",
    "nc -l 9999\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:20:30.019650Z",
     "start_time": "2024-05-07T13:20:29.212424Z"
    }
   },
   "source": [
    "df = (\n",
    "    spark.readStream\n",
    "    .format(\"socket\")\n",
    "    .option(\"host\", \"localhost\")\n",
    "    .option(\"port\", 9999)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "print(df)\n",
    "print(df.isStreaming)\n",
    "print(df.printSchema())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[value: string]\n",
      "True\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dupa cum putem observa, `df` este un simplu DataFrame, dar care are `streaming` setat pe `True`. Acesta nu contine inca date, deoarece nu am pornit stream-ul.\n",
    "\n",
    "**Important**: Nu se poate apela metoda `show` pe un DataFrame de streaming, deoarece aceasta metoda nu este suportata. In schimb, putem apela metoda `printSchema` pentru a afisa schema DataFrame-ului.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:21:04.214284Z",
     "start_time": "2024-05-07T13:21:04.209405Z"
    }
   },
   "source": "df.isStreaming",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:22:36.656570Z",
     "start_time": "2024-05-07T13:21:40.305966Z"
    }
   },
   "source": [
    "query = (\n",
    "    df.writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 8\u001B[0m\n\u001B[1;32m      1\u001B[0m query \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m      2\u001B[0m     df\u001B[38;5;241m.\u001B[39mwriteStream\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconsole\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m      6\u001B[0m )\n\u001B[0;32m----> 8\u001B[0m query\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:221\u001B[0m, in \u001B[0;36mStreamingQuery.awaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination(\u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m))\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 221\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1314\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1036\u001B[0m connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_connection()\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1038\u001B[0m     response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1039\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m binary:\n\u001B[1;32m   1040\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection_guard(connection)\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001B[0m, in \u001B[0;36mClientServerConnection.send_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    510\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 511\u001B[0m         answer \u001B[38;5;241m=\u001B[39m smart_decode(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream\u001B[38;5;241m.\u001B[39mreadline()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m    512\u001B[0m         logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnswer received: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(answer))\n\u001B[1;32m    513\u001B[0m         \u001B[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001B[39;00m\n\u001B[1;32m    514\u001B[0m         \u001B[38;5;66;03m# answer before the socket raises an error.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/socket.py:706\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    704\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 706\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39mrecv_into(b)\n\u001B[1;32m    707\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    708\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odata ce s-a pornit stream-ul, putem observa ca datele sunt citite si afisate in consola 'imediat' ce sunt introduse in socket.\n",
    "De exemplu, daca in socket au fost introduse datele:\n",
    "\n",
    "```bash\n",
    "ana are mere\n",
    "ma duc la magazin\n",
    "```\n",
    "\n",
    "In outup se va afisa:\n",
    "\n",
    "```bash\n",
    " Batch: 0\n",
    " -------------------------------------------\n",
    " +-----+\n",
    " |value|\n",
    " +-----+\n",
    " +-----+\n",
    "\n",
    " -------------------------------------------\n",
    " Batch: 1\n",
    " -------------------------------------------\n",
    " +------------+\n",
    " |       value|\n",
    " +------------+\n",
    " |ana are mere|\n",
    " +------------+\n",
    "\n",
    " -------------------------------------------\n",
    " Batch: 2\n",
    " -------------------------------------------\n",
    " +-----------------+\n",
    " |            value|\n",
    " +-----------------+\n",
    " |ma duc la magazin|\n",
    " +-----------------+\n",
    "```\n",
    "\n",
    "Batch 0 este gol, deoarece nu am introdus date in socket inainte de a porni stream-ul.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:23:03.114732Z",
     "start_time": "2024-05-07T13:23:02.856635Z"
    }
   },
   "source": [
    "# stop the query\n",
    "query.stop()"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datele introduse in socket nu trebuie sa fie neaparat string-uri, ci pot fi si alte tipuri de date, de exemplu json. Aceste tipuri de date sunt citite ca string-uri si trebuie sa fie parsate inainte de a fi folosite.\n",
    "\n",
    "Opriti socketul prin comanda `Ctrl + C` si redeschideti-l pentru a putea rula urmatoarul exemplu.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:24:20.778934Z",
     "start_time": "2024-05-07T13:24:20.759019Z"
    }
   },
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "json_schema = T.StructType([\n",
    "    T.StructField(\"name\", T.StringType()),\n",
    "    T.StructField(\"age\", T.IntegerType()),\n",
    "])\n",
    "\n",
    "df = (\n",
    "    spark.readStream\n",
    "    .format(\"socket\")\n",
    "    .option(\"host\", \"localhost\")\n",
    "    .option(\"port\", 9999)\n",
    "    .load()\n",
    ")\n",
    "df.printSchema()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:24:40.747531Z",
     "start_time": "2024-05-07T13:24:40.629417Z"
    }
   },
   "source": [
    "json_df = df.select(F.from_json(\"value\", json_schema).alias(\"parsed_value\"))\n",
    "json_df.printSchema()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- parsed_value: struct (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:25:03.872301Z",
     "start_time": "2024-05-07T13:25:03.850584Z"
    }
   },
   "source": [
    "final_df = json_df.select(\"parsed_value.*\")\n",
    "final_df.printSchema()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:26:21.807445Z",
     "start_time": "2024-05-07T13:25:54.006258Z"
    }
   },
   "source": [
    "query = (\n",
    "    final_df.writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 8\u001B[0m\n\u001B[1;32m      1\u001B[0m query \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m      2\u001B[0m     final_df\u001B[38;5;241m.\u001B[39mwriteStream\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconsole\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m      6\u001B[0m )\n\u001B[0;32m----> 8\u001B[0m query\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:221\u001B[0m, in \u001B[0;36mStreamingQuery.awaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination(\u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m))\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 221\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1314\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1036\u001B[0m connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_connection()\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1038\u001B[0m     response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1039\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m binary:\n\u001B[1;32m   1040\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection_guard(connection)\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001B[0m, in \u001B[0;36mClientServerConnection.send_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    510\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 511\u001B[0m         answer \u001B[38;5;241m=\u001B[39m smart_decode(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream\u001B[38;5;241m.\u001B[39mreadline()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m    512\u001B[0m         logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnswer received: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(answer))\n\u001B[1;32m    513\u001B[0m         \u001B[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001B[39;00m\n\u001B[1;32m    514\u001B[0m         \u001B[38;5;66;03m# answer before the socket raises an error.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/socket.py:706\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    704\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 706\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39mrecv_into(b)\n\u001B[1;32m    707\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    708\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daca scriem in socket datele:\n",
    "\n",
    "```bash\n",
    "{\"name\": \"Ana\", \"age\": 20}\n",
    "{\"name\": \"Maria\", \"age\": 30}\n",
    "```\n",
    "\n",
    "Vom obtine in output:\n",
    "\n",
    "```bash\n",
    "Batch: 0\n",
    "-------------------------------------------\n",
    "+----+---+\n",
    "|name|age|\n",
    "+----+---+\n",
    "+----+---+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 1\n",
    "-------------------------------------------\n",
    "+----+---+\n",
    "|name|age|\n",
    "+----+---+\n",
    "| Ana| 20|\n",
    "+----+---+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 2\n",
    "-------------------------------------------\n",
    "+-----+---+\n",
    "| name|age|\n",
    "+-----+---+\n",
    "|Maria| 30|\n",
    "+-----+---+\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark UI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pentru fiecare micro-batch, Spark creeaza un nou job pentru procesarea datelor. Daca mergem in Spark UI, la sectiunes [Jobs](http://localhost:4040/jobs/), putem observa joburile create de Spark pentru procesare. Ar trebui sa vedem 6 joburi, 3 pentru primul exemplu si 3 pentru al doilea exemplu.\n",
    "\n",
    "![Jobs Image](./images/sparkUI/jobsReading.png)\n",
    "\n",
    "Numarul de taskuri folosite este 16, adica numarul de core-uri disponibile. Acest numar se poate vedea ruland\n",
    "\n",
    "```python\n",
    "spark.sparkContext.defaultParallelism\n",
    "```\n",
    "\n",
    "Odata ce cel putin o operatie de tip stream este rulata in cadrul Spark UI, va fi disponibila sectiune [Structured Streaming](http://localhost:4040/StreamingQuery/) unde putem vedea informatii despre streamuri. In cazul nostru ar trebuie sa se vada pagina\n",
    "\n",
    "[![Streaming Query Image](./images/sparkUI/streamingInfo.png)](http://localhost:4040/StreamingQuery/)\n",
    "\n",
    "Putem observa ca avem un stream activ si unul terminat, deoarece pentru al doilea stream nu am apelat inca `query.stop()` pentru a-l opri. De asemena, daca se apasa pe fiecare stream se vor putea vedea date statistice ale acestuia.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:28:20.772579Z",
     "start_time": "2024-05-07T13:28:20.769059Z"
    }
   },
   "source": "query.stop()",
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading From Files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In continuare, voi prezenta cum putem citi datele de streaming din fisiere. Pentru a face acest lucru, vom folosi metoda `readStream` a obiectului `SparkSession`. ReadStream se comporta similar cu metoda `read` a obiectului `SparkSession`, dar in loc sa returneze un DataFrame static, returneaza un DataFrame de streaming. In cazul nostru, voi folosi un fisier de tip json multiline. Datele din acesta pot fi gasite in folderul `data/devices` din acest repository.\n",
    "\n",
    "Pentru a citi din fisiere, Spark va urmari un director si va citi toate fisierele din acel director, o data ce sunt adaugate.\n",
    "\n",
    "Schema fisierelor poate fi data in mod manual, sau se poate activa optiunea de inferare a acesteia, setand configurarea: `spark.conf.set(\"spark.sql.streaming.schemaInference\", True)`. Pentru ca aceasta configurare sa functioneze trebuie sa existe cel putin un fisier in directorul de citire.\n",
    "\n",
    "Fisierele pe care le vom citi sunt de tip json multiline, asa ca trebuie sa setam optiunea `multiline` pe `True`, iar un exemplu de intrare este:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"eventId\": \"e3cb26d3-41b2-49a2-84f3-0156ed8d7502\",\n",
    "  \"eventOffset\": 10001,\n",
    "  \"eventPublisher\": \"device\",\n",
    "  \"customerId\": \"CI00103\",\n",
    "  \"data\": {\n",
    "    \"devices\": [\n",
    "      {\n",
    "        \"deviceId\": \"D001\",\n",
    "        \"temperature\": 15,\n",
    "        \"measure\": \"C\",\n",
    "        \"status\": \"ERROR\"\n",
    "      },\n",
    "      {\n",
    "        \"deviceId\": \"D002\",\n",
    "        \"temperature\": 16,\n",
    "        \"measure\": \"C\",\n",
    "        \"status\": \"SUCCESS\"\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  \"eventTime\": \"2023-01-05 11:13:53.643364\"\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.sql.streaming.schemaInference\", True)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:29:46.323879Z",
     "start_time": "2024-05-07T13:29:46.319373Z"
    }
   },
   "source": [
    "import os\n",
    "data_path = os.path.join(os.getcwd(), \"referat\")\n",
    "data_path"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/referat'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:30:36.043070Z",
     "start_time": "2024-05-07T13:30:36.038632Z"
    }
   },
   "source": [
    "import pyspark.sql.types as T\n",
    "\n",
    "device_schema = T.StructType([\n",
    "    T.StructField(\"deviceId\", T.StringType(), True),\n",
    "    T.StructField(\"temperature\", T.IntegerType(), True),\n",
    "    T.StructField(\"measure\", T.StringType(), True),\n",
    "    T.StructField(\"status\", T.StringType(), True)\n",
    "])\n",
    "\n",
    "json_schema = T.StructType([\n",
    "    T.StructField(\"eventId\", T.StringType(), True),\n",
    "    T.StructField(\"eventOffset\", T.IntegerType(), True),\n",
    "    T.StructField(\"eventPublisher\", T.StringType(), True),\n",
    "    T.StructField(\"customerId\", T.StringType(), True),\n",
    "    T.StructField(\"data\", T.StructType([\n",
    "        T.StructField(\"devices\", T.ArrayType(device_schema), True)\n",
    "    ]), True),\n",
    "    T.StructField(\"eventTime\", T.TimestampType(), True)\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:30:49.701488Z",
     "start_time": "2024-05-07T13:30:49.697025Z"
    }
   },
   "source": [
    "def create_final_df(df):\n",
    "    return df.withColumn(\n",
    "        \"data_devices\", F.explode('data.devices'))\\\n",
    "        .withColumn('deviceId', F.col('data_devices.deviceId'))\\\n",
    "        .withColumn('measure', F.col('data_devices.measure'))\\\n",
    "        .withColumn('status', F.col('data_devices.status'))\\\n",
    "        .withColumn('temperature', F.col('data_devices.temperature'))\\\n",
    "        .drop('data_devices').drop('data')"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:30:57.587583Z",
     "start_time": "2024-05-07T13:30:57.351002Z"
    }
   },
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "read_df = spark.readStream\\\n",
    "    .json(data_path+'/input/devices/', multiLine=True, schema=json_schema)\n",
    "\n",
    "\n",
    "final_df = create_final_df(read_df)\n",
    "\n",
    "\n",
    "final_df.printSchema()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventOffset: integer (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- eventTime: timestamp (nullable = true)\n",
      " |-- deviceId: string (nullable = true)\n",
      " |-- measure: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- temperature: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:33:40.615478Z",
     "start_time": "2024-05-07T13:32:43.056080Z"
    }
   },
   "source": [
    "query = (\n",
    "    final_df.writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 8\u001B[0m\n\u001B[1;32m      1\u001B[0m query \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m      2\u001B[0m     final_df\u001B[38;5;241m.\u001B[39mwriteStream\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconsole\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m      6\u001B[0m )\n\u001B[0;32m----> 8\u001B[0m query\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:221\u001B[0m, in \u001B[0;36mStreamingQuery.awaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination(\u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m))\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 221\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1314\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1036\u001B[0m connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_connection()\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1038\u001B[0m     response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1039\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m binary:\n\u001B[1;32m   1040\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection_guard(connection)\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001B[0m, in \u001B[0;36mClientServerConnection.send_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    510\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 511\u001B[0m         answer \u001B[38;5;241m=\u001B[39m smart_decode(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream\u001B[38;5;241m.\u001B[39mreadline()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m    512\u001B[0m         logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnswer received: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(answer))\n\u001B[1;32m    513\u001B[0m         \u001B[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001B[39;00m\n\u001B[1;32m    514\u001B[0m         \u001B[38;5;66;03m# answer before the socket raises an error.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/socket.py:706\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    704\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 706\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39mrecv_into(b)\n\u001B[1;32m    707\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    708\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:33:46.909795Z",
     "start_time": "2024-05-07T13:33:46.897065Z"
    }
   },
   "source": [
    "query.stop()"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daca vom copia prima data fisierul `device_01.json` in directorul de citire, iar apoi fisierele `device_02.json` si `device_03.json` impreuna, vom observa ca by default Spark nu citeste fiser cu fiser, ci citeste toate fisierele din directorul de citire, odata ce sunt adaugate.\n",
    "\n",
    "Astfel, in logurile `bd-jupyter-notebook-lab` vom avea 2 batchuri, unul pentru fisierul `device_01.json` si unul pentru fisierele `device_02.json` si `device_03.json`:\n",
    "\n",
    "```bash\n",
    "Batch: 0\n",
    "-------------------------------------------\n",
    "+--------------------+-----------+--------------+----------+--------------------+--------+-------+-------+-----------+\n",
    "|             eventId|eventOffset|eventPublisher|customerId|           eventTime|deviceId|measure| status|temperature|\n",
    "+--------------------+-----------+--------------+----------+--------------------+--------+-------+-------+-----------+\n",
    "|e3cb26d3-41b2-49a...|      10001|        device|   CI00103|2023-01-05 11:13:...|    D001|      C|  ERROR|         15|\n",
    "|e3cb26d3-41b2-49a...|      10001|        device|   CI00103|2023-01-05 11:13:...|    D002|      C|SUCCESS|         16|\n",
    "+--------------------+-----------+--------------+----------+--------------------+--------+-------+-------+-----------+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 1\n",
    "-------------------------------------------\n",
    "+--------------------+-----------+--------------+----------+--------------------+--------+-------+-------+-----------+\n",
    "|             eventId|eventOffset|eventPublisher|customerId|           eventTime|deviceId|measure| status|temperature|\n",
    "+--------------------+-----------+--------------+----------+--------------------+--------+-------+-------+-----------+\n",
    "|1450324a-c546-417...|      10038|        device|   CI00101|2023-01-05 11:13:...|    D004|      C|SUCCESS|         20|\n",
    "|1450324a-c546-417...|      10038|        device|   CI00101|2023-01-05 11:13:...|    D004|      C|SUCCESS|          1|\n",
    "|1450324a-c546-417...|      10038|        device|   CI00101|2023-01-05 11:13:...|    D002|      C|SUCCESS|         21|\n",
    "|aa90011f-3967-496...|      10003|        device|   CI00108|2023-01-05 11:13:...|    D004|      C|SUCCESS|         16|\n",
    "+--------------------+-----------+--------------+----------+--------------------+--------+-------+-------+-----------+\n",
    "```\n",
    "\n",
    "Daca dorim sa citim cate un numar specific indiferent de cate fisiere adaugam deodata putem folosi optiunea `maxFilesPerTrigger` pentru a specifica cate fisiere sa fie citite la fiecare trigger. De exemplu, daca dorim sa citim cate un fisier la fiecare trigger, putem seta optiunea `maxFilesPerTrigger` pe 1.\n",
    "\n",
    "Inainte de a ilustra acest exemplu, trebuie sa stergem fisierele din directorul de citire.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:35:47.849116Z",
     "start_time": "2024-05-07T13:35:23.330681Z"
    }
   },
   "source": [
    "read_df = spark.readStream.option(\"maxFilesPerTrigger\", 1)\\\n",
    "    .json(data_path+'/input/devices/', multiLine=True, schema=json_schema)\n",
    "\n",
    "final_df = create_final_df(read_df)\n",
    "\n",
    "query = (\n",
    "    final_df.writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 13\u001B[0m\n\u001B[1;32m      4\u001B[0m final_df \u001B[38;5;241m=\u001B[39m create_final_df(read_df)\n\u001B[1;32m      6\u001B[0m query \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m      7\u001B[0m     final_df\u001B[38;5;241m.\u001B[39mwriteStream\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconsole\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m     11\u001B[0m )\n\u001B[0;32m---> 13\u001B[0m query\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:221\u001B[0m, in \u001B[0;36mStreamingQuery.awaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination(\u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m))\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 221\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1314\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1036\u001B[0m connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_connection()\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1038\u001B[0m     response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1039\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m binary:\n\u001B[1;32m   1040\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection_guard(connection)\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001B[0m, in \u001B[0;36mClientServerConnection.send_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    510\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 511\u001B[0m         answer \u001B[38;5;241m=\u001B[39m smart_decode(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream\u001B[38;5;241m.\u001B[39mreadline()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m    512\u001B[0m         logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnswer received: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(answer))\n\u001B[1;32m    513\u001B[0m         \u001B[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001B[39;00m\n\u001B[1;32m    514\u001B[0m         \u001B[38;5;66;03m# answer before the socket raises an error.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/socket.py:706\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    704\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 706\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39mrecv_into(b)\n\u001B[1;32m    707\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    708\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:35:51.596442Z",
     "start_time": "2024-05-07T13:35:51.571312Z"
    }
   },
   "source": [
    "query.stop()"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daca repetam actiunile anterioare vom vedea ca de data aceasta vor fi afisate 3 batch-uri, unul pentru fiecare fisier:\n",
    "\n",
    "```bash\n",
    "Batch: 0\n",
    "-------------------------------------------\n",
    "+--------------------+-----------+--------------+----------+--------------------+--------+-------+-------+-----------+\n",
    "|             eventId|eventOffset|eventPublisher|customerId|           eventTime|deviceId|measure| status|temperature|\n",
    "+--------------------+-----------+--------------+----------+--------------------+--------+-------+-------+-----------+\n",
    "|e3cb26d3-41b2-49a...|      10001|        device|   CI00103|2023-01-05 11:13:...|    D001|      C|  ERROR|         15|\n",
    "|e3cb26d3-41b2-49a...|      10001|        device|   CI00103|2023-01-05 11:13:...|    D002|      C|SUCCESS|         16|\n",
    "+--------------------+-----------+--------------+----------+--------------------+--------+-------+-------+-----------+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 1\n",
    "-------------------------------------------\n",
    "+--------------------+-----------+--------------+----------+--------------------+--------+-------+-------+-----------+\n",
    "|             eventId|eventOffset|eventPublisher|customerId|           eventTime|deviceId|measure| status|temperature|\n",
    "+--------------------+-----------+--------------+----------+--------------------+--------+-------+-------+-----------+\n",
    "|aa90011f-3967-496...|      10003|        device|   CI00108|2023-01-05 11:13:...|    D004|      C|SUCCESS|         16|\n",
    "+--------------------+-----------+--------------+----------+--------------------+--------+-------+-------+-----------+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 2\n",
    "-------------------------------------------\n",
    "+--------------------+-----------+--------------+----------+--------------------+--------+-------+-------+-----------+\n",
    "|             eventId|eventOffset|eventPublisher|customerId|           eventTime|deviceId|measure| status|temperature|\n",
    "+--------------------+-----------+--------------+----------+--------------------+--------+-------+-------+-----------+\n",
    "|1450324a-c546-417...|      10038|        device|   CI00101|2023-01-05 11:13:...|    D004|      C|SUCCESS|         20|\n",
    "|1450324a-c546-417...|      10038|        device|   CI00101|2023-01-05 11:13:...|    D004|      C|SUCCESS|          1|\n",
    "|1450324a-c546-417...|      10038|        device|   CI00101|2023-01-05 11:13:...|    D002|      C|SUCCESS|         21|\n",
    "+--------------------+-----------+--------------+----------+--------------------+--------+-------+-------+-----------+\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De asemenea, la citire se poate specifica si optiunea `cleanSoruce`, care poate avea urmatoarele valori:\n",
    "\n",
    "- \"off\" - este default si nu modifica directorul de citire\n",
    "- \"delete\" - sterge fisierele citite\n",
    "- \"archive\" - muta fisierele citite in folderul specificat de optiunea `sourceArchiveDir`. Mutarea se face doar in momentul in care sunt introduse fisere noi. De exemplu, daca introducem `device_01.json` si-l citim, acesta nu va fi mutat decat dupa ce am citit si `device_02.json`.\n",
    "\n",
    "Inainte de a ilustra acest exemplu, trebuie sa stergem fisierele din directorul de citire.\n",
    "\n",
    "In continuare vom vedea optiunea `archive`:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:36:46.733696Z",
     "start_time": "2024-05-07T13:36:46.729798Z"
    }
   },
   "source": [
    "sourceArchiveDir = data_path+'/output/archive/devices'\n",
    "sourceArchiveDir"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/referat/output/archive/devices'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:38:56.919049Z",
     "start_time": "2024-05-07T13:37:34.981856Z"
    }
   },
   "source": [
    "read_df = spark.readStream.option(\"maxFilesPerTrigger\", 1)\\\n",
    "    .option(\"cleanSource\", \"archive\")\\\n",
    "    .option(\"sourceArchiveDir\", sourceArchiveDir)\\\n",
    "    .json(data_path+'/input/devices/', multiLine=True, schema=json_schema)\n",
    "\n",
    "final_df = create_final_df(read_df)\n",
    "\n",
    "query = (\n",
    "    final_df.writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[25], line 15\u001B[0m\n\u001B[1;32m      6\u001B[0m final_df \u001B[38;5;241m=\u001B[39m create_final_df(read_df)\n\u001B[1;32m      8\u001B[0m query \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m      9\u001B[0m     final_df\u001B[38;5;241m.\u001B[39mwriteStream\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconsole\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m     13\u001B[0m )\n\u001B[0;32m---> 15\u001B[0m query\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:221\u001B[0m, in \u001B[0;36mStreamingQuery.awaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination(\u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m))\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 221\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1314\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1036\u001B[0m connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_connection()\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1038\u001B[0m     response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1039\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m binary:\n\u001B[1;32m   1040\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection_guard(connection)\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001B[0m, in \u001B[0;36mClientServerConnection.send_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    510\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 511\u001B[0m         answer \u001B[38;5;241m=\u001B[39m smart_decode(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream\u001B[38;5;241m.\u001B[39mreadline()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m    512\u001B[0m         logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnswer received: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(answer))\n\u001B[1;32m    513\u001B[0m         \u001B[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001B[39;00m\n\u001B[1;32m    514\u001B[0m         \u001B[38;5;66;03m# answer before the socket raises an error.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/socket.py:706\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    704\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 706\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39mrecv_into(b)\n\u001B[1;32m    707\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    708\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daca vom aduaga fisierele unul cate unul, dupa adaugarea urmatorului, la refresh se poate vedea ca cel dinainte a disparut din folderul de cititre, iar acesta va aparea in cel de arhivare.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:39:01.074138Z",
     "start_time": "2024-05-07T13:39:01.052899Z"
    }
   },
   "source": [
    "query.stop()"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Data And Windowing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In continuare, voi prezenta cum putem scrie datele de streaming in sink. Pentru a face acest lucru, vom folosi metoda `writeStream` a obiectului DataFrame. WriteStream se comporta similar cu metoda `write`, dar in loc sa scrie datele in mod static, le scrie in mod continuu.\n",
    "\n",
    "Metodele de scriere ale output-ului sunt:\n",
    "\n",
    "- _Complete Mode_ : intregul rezultat al tabelului este scris in sink la fiecare trigger. Acest mod este folosit pentru agregari, deoarece rezultatul complet al agregarii este necesar pentru a calcula agregarile corect.\n",
    "- _Append Mode_ : doar noile linii adaugate in tabel sunt scrise in sink la fiecare trigger. Acest mod este folosit pentru a scrie datele in sink fara a le modifica si suporta doar agregarile care au un watermark pentru a sti cand datele devin nemodificabile. Acest mod este compatibil cu sinkurile care suporta doar adaugarea de date fara a le modifica pe cele existente.\n",
    "- _Update Mode_ : doar liniile modificate in tabel sunt scrise in sink la fiecare trigger. Acest mod este similar cu Append Mode, daca nu se fac agregari, dar daca se fac agregari, atunci se scriu doar liniile modificate. Acest mod, este compatibil doar cu sinkurile care suporta actualizarea si stergerea datelor in mod atomic.\n",
    "\n",
    "Pentru a ilustra modurile de scriere, vom folosi sink-ul `console`, care va afisa datele in loggurile containerului `bd-jupyter-notebook-lab`. Datele de intrare vor fi citite din socket, si vor putea fi gasite in folderul `data/words` din acest repository.\n",
    "\n",
    "Un element de intrare este:\n",
    "\n",
    "```json\n",
    "{ \"timestamp\": \"2024-04-06 10:05:00\", \"word\": \"Streaming\" }\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:40:51.687551Z",
     "start_time": "2024-05-07T13:40:51.684185Z"
    }
   },
   "source": [
    "word_schema = T.StructType([\n",
    "    T.StructField(\"word\", T.StringType(), True),\n",
    "    T.StructField(\"timestamp\", T.TimestampType(), True)\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:41:00.989909Z",
     "start_time": "2024-05-07T13:41:00.925032Z"
    }
   },
   "source": [
    "read_df = (\n",
    "    spark.readStream\n",
    "    .format(\"socket\")\n",
    "    .option(\"host\", \"localhost\")\n",
    "    .option(\"port\", 9999)\n",
    "    .load()\n",
    "    .withColumn(\"parsed_value\", F.from_json(\"value\", word_schema))\n",
    "    .select('parsed_value.*')\n",
    "    .filter(F.col('word').isNotNull())\n",
    "    .groupBy('word')\n",
    "    .agg(F.count('word').alias('count'))\n",
    ")\n",
    "\n",
    "read_df.printSchema()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- word: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Mode\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:42:33.045750Z",
     "start_time": "2024-05-07T13:41:25.204284Z"
    }
   },
   "source": [
    "queryComplete = (\n",
    "    read_df.writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"complete\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "queryComplete.awaitTermination()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[30], line 8\u001B[0m\n\u001B[1;32m      1\u001B[0m queryComplete \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m      2\u001B[0m     read_df\u001B[38;5;241m.\u001B[39mwriteStream\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconsole\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcomplete\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m      6\u001B[0m )\n\u001B[0;32m----> 8\u001B[0m queryComplete\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:221\u001B[0m, in \u001B[0;36mStreamingQuery.awaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination(\u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m))\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 221\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1314\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1036\u001B[0m connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_connection()\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1038\u001B[0m     response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1039\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m binary:\n\u001B[1;32m   1040\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection_guard(connection)\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001B[0m, in \u001B[0;36mClientServerConnection.send_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    510\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 511\u001B[0m         answer \u001B[38;5;241m=\u001B[39m smart_decode(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream\u001B[38;5;241m.\u001B[39mreadline()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m    512\u001B[0m         logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnswer received: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(answer))\n\u001B[1;32m    513\u001B[0m         \u001B[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001B[39;00m\n\u001B[1;32m    514\u001B[0m         \u001B[38;5;66;03m# answer before the socket raises an error.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/socket.py:706\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    704\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 706\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39mrecv_into(b)\n\u001B[1;32m    707\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    708\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daca in socket avem intrarile :\n",
    "\n",
    "```bash\n",
    "{\"timestamp\":\"2024-04-06 10:05:00\",\"word\":\"Streaming\"}\n",
    "{\"timestamp\":\"2024-04-06 10:40:00\",\"word\":\"Hello\"}\n",
    "{\"timestamp\":\"2024-04-06 10:20:00\",\"word\":\"Spark\"}\n",
    "{\"timestamp\":\"2024-04-06 10:48:00\",\"word\":\"Streaming\"}\n",
    "{\"timestamp\":\"2024-04-06 10:14:00\",\"word\":\"Streaming\"}\n",
    "{\"timestamp\":\"2024-04-06 10:16:00\",\"word\":\"Hadoop\"}\n",
    "```\n",
    "\n",
    "In logurile containerului vom vedea cele 7 batchuri, care vor contine toate datele indiferent de liniile modificate:\n",
    "\n",
    "```bash\n",
    "Batch: 0\n",
    "-------------------------------------------\n",
    "+----+-----+\n",
    "|word|count|\n",
    "+----+-----+\n",
    "+----+-----+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 1\n",
    "-------------------------------------------\n",
    "+---------+-----+\n",
    "|     word|count|\n",
    "+---------+-----+\n",
    "|Streaming|    1|\n",
    "+---------+-----+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 2\n",
    "-------------------------------------------\n",
    "+---------+-----+\n",
    "|     word|count|\n",
    "+---------+-----+\n",
    "|    Hello|    1|\n",
    "|Streaming|    1|\n",
    "+---------+-----+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 3\n",
    "-------------------------------------------\n",
    "+---------+-----+\n",
    "|     word|count|\n",
    "+---------+-----+\n",
    "|    Hello|    1|\n",
    "|Streaming|    1|\n",
    "|    Spark|    1|\n",
    "+---------+-----+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 4\n",
    "-------------------------------------------\n",
    "+---------+-----+\n",
    "|     word|count|\n",
    "+---------+-----+\n",
    "|    Hello|    1|\n",
    "|Streaming|    2|\n",
    "|    Spark|    1|\n",
    "+---------+-----+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 5\n",
    "-------------------------------------------\n",
    "+---------+-----+\n",
    "|     word|count|\n",
    "+---------+-----+\n",
    "|    Hello|    1|\n",
    "|Streaming|    3|\n",
    "|    Spark|    1|\n",
    "+---------+-----+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 6\n",
    "-------------------------------------------\n",
    "+---------+-----+\n",
    "|     word|count|\n",
    "+---------+-----+\n",
    "|    Hello|    1|\n",
    "|Streaming|    3|\n",
    "|    Spark|    1|\n",
    "|   Hadoop|    1|\n",
    "+---------+-----+\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:42:39.975481Z",
     "start_time": "2024-05-07T13:42:39.889107Z"
    }
   },
   "source": [
    "queryComplete.stop()"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Mode\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:43:01.286623Z",
     "start_time": "2024-05-07T13:42:56.594602Z"
    }
   },
   "source": [
    "queryUpdate = (\n",
    "    read_df.writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"update\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "queryUpdate.awaitTermination()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[32], line 8\u001B[0m\n\u001B[1;32m      1\u001B[0m queryUpdate \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m      2\u001B[0m     read_df\u001B[38;5;241m.\u001B[39mwriteStream\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconsole\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mupdate\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m      6\u001B[0m )\n\u001B[0;32m----> 8\u001B[0m queryUpdate\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:221\u001B[0m, in \u001B[0;36mStreamingQuery.awaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination(\u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m))\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 221\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1314\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1036\u001B[0m connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_connection()\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1038\u001B[0m     response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1039\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m binary:\n\u001B[1;32m   1040\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection_guard(connection)\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001B[0m, in \u001B[0;36mClientServerConnection.send_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    510\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 511\u001B[0m         answer \u001B[38;5;241m=\u001B[39m smart_decode(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream\u001B[38;5;241m.\u001B[39mreadline()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m    512\u001B[0m         logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnswer received: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(answer))\n\u001B[1;32m    513\u001B[0m         \u001B[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001B[39;00m\n\u001B[1;32m    514\u001B[0m         \u001B[38;5;66;03m# answer before the socket raises an error.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/socket.py:706\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    704\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 706\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39mrecv_into(b)\n\u001B[1;32m    707\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    708\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daca vom introduce in socket aceleasi intrari vom vedea ca in loguri vom avea dor liniile care s-au modificat:\n",
    "\n",
    "```bash\n",
    "Batch: 0\n",
    "-------------------------------------------\n",
    "+----+-----+\n",
    "|word|count|\n",
    "+----+-----+\n",
    "+----+-----+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 1\n",
    "-------------------------------------------\n",
    "+---------+-----+\n",
    "|     word|count|\n",
    "+---------+-----+\n",
    "|Streaming|    1|\n",
    "+---------+-----+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 2\n",
    "-------------------------------------------\n",
    "+-----+-----+\n",
    "| word|count|\n",
    "+-----+-----+\n",
    "|Hello|    1|\n",
    "+-----+-----+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 3\n",
    "-------------------------------------------\n",
    "+-----+-----+\n",
    "| word|count|\n",
    "+-----+-----+\n",
    "|Spark|    1|\n",
    "+-----+-----+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 4\n",
    "-------------------------------------------\n",
    "+---------+-----+\n",
    "|     word|count|\n",
    "+---------+-----+\n",
    "|Streaming|    2|\n",
    "+---------+-----+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 5\n",
    "-------------------------------------------\n",
    "+---------+-----+\n",
    "|     word|count|\n",
    "+---------+-----+\n",
    "|Streaming|    3|\n",
    "+---------+-----+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 6\n",
    "-------------------------------------------\n",
    "+------+-----+\n",
    "|  word|count|\n",
    "+------+-----+\n",
    "|Hadoop|    1|\n",
    "+------+-----+\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:43:05.111698Z",
     "start_time": "2024-05-07T13:43:05.090734Z"
    }
   },
   "source": [
    "queryUpdate.stop()"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window Operations And Append Mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inainte de a prezenta modul de scriere `update`, vom vorbi despre operatiile de tip `window`:\n",
    "\n",
    "- _Tumbling Window_: este un tip de fereastra care se imparte in bucati de o anumita durata, de exemplu, daca avem o fereastra de 5 minute, aceasta va imparti datele dupa o coloana temporala in grupuri de 5 minute. Daca avem 3 inputuri la ora 10:00, 10:01, 10:06, se vor crea doua ferestre de tipul [10:00, 10:05) si [10:05, 10:10).Primele doua inputuri vor fi in prima fereastra si cel de-al treilea in cea de-a doua.\n",
    "- _Sliding Window_: este un tip de fereastra care se imparte in bucati de o anumita durata, dar care se suprapun. De exemplu, daca avem o fereastra de 5 minute si un slide de 2 minute, aceasta se va imparti datele dupa o coloana temporal in grupuri de 5 minute care se suprapun. Daca avem 3 inputuri la ora 10:00, 10:01, 10:06, se vor crea trei ferestre de tipul [10:00, 10:05), [10:02, 10:07) si [10:04, 10:09). Primele doua inputuri vor fi in prima fereastra, cel de-al doilea in a doua si cel de-al treilea in a treia.\n",
    "- _Watermark_: este un mecanism care ne ajuta sa gestionam datele care sunt in intarziere. Daca un eveniment ajunge in sistem cu o intarziere mai mare decat watermark-ul, acesta nu va fi luat in considerare. De exemplu, daca avem un watermark de 5 minute atunci un eveniment intarziat va fi procesat doar daca ajunge mai devreme de max(timestamp evenimente)-5 minute. De asemenea, Spark va folosi watermarkul pentru a sterge din memorie ferestrele care nu mai pot fi actualizate.\n",
    "- _Session Window_: este un tip de fereastra care se imparte in bucati de o anumita durata, dar care se extind in functie de timpul de inactivitate al datelor. Daca nu se primesc date timp de o anumita durata, fereastra se inchide. Daca se primesc date dupa ce fereastra s-a inchis, se va deschide o noua fereastra. Exista restrictii suplimentare: `update mode` nu este suportat ca output, iar in groupBy trebuie sa existe cel putin inca o coloana in afara de `session_window`\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:50:18.873490Z",
     "start_time": "2024-05-07T13:50:18.793995Z"
    }
   },
   "source": [
    "# Thumbling window\n",
    "\n",
    "read_df = (\n",
    "    spark.readStream\n",
    "    .format(\"socket\")\n",
    "    .option(\"host\", \"localhost\")\n",
    "    .option(\"port\", 9999)\n",
    "    .load()\n",
    "    .withColumn(\"parsed_value\", F.from_json(\"value\", word_schema))\n",
    "    .select('parsed_value.*')\n",
    "    .filter(F.col('word').isNotNull())\n",
    "    .groupBy('word', F.window('timestamp', '15 minute'))\n",
    "    .agg(F.count('word').alias('count'))\n",
    ")\n",
    "read_df.printSchema()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- word: string (nullable = true)\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:50:45.759412Z",
     "start_time": "2024-05-07T13:50:45.744073Z"
    }
   },
   "source": [
    "final_df = read_df.select('window.*', 'word', 'count')\n",
    "\n",
    "final_df.printSchema()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- start: timestamp (nullable = true)\n",
      " |-- end: timestamp (nullable = true)\n",
      " |-- word: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:52:27.786353Z",
     "start_time": "2024-05-07T13:51:00.253551Z"
    }
   },
   "source": [
    "thumbling_query = (\n",
    "    final_df.writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"update\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "thumbling_query.awaitTermination()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[36], line 8\u001B[0m\n\u001B[1;32m      1\u001B[0m thumbling_query \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m      2\u001B[0m     final_df\u001B[38;5;241m.\u001B[39mwriteStream\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconsole\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mupdate\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m      6\u001B[0m )\n\u001B[0;32m----> 8\u001B[0m thumbling_query\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:221\u001B[0m, in \u001B[0;36mStreamingQuery.awaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination(\u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m))\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 221\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1314\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1036\u001B[0m connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_connection()\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1038\u001B[0m     response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1039\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m binary:\n\u001B[1;32m   1040\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection_guard(connection)\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001B[0m, in \u001B[0;36mClientServerConnection.send_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    510\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 511\u001B[0m         answer \u001B[38;5;241m=\u001B[39m smart_decode(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream\u001B[38;5;241m.\u001B[39mreadline()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m    512\u001B[0m         logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnswer received: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(answer))\n\u001B[1;32m    513\u001B[0m         \u001B[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001B[39;00m\n\u001B[1;32m    514\u001B[0m         \u001B[38;5;66;03m# answer before the socket raises an error.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/socket.py:706\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    704\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 706\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39mrecv_into(b)\n\u001B[1;32m    707\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    708\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daca in socket introducem inputurile in ordine:\n",
    "\n",
    "```bash\n",
    "{\"timestamp\":\"2024-04-06 10:05:00\",\"word\":\"Streaming\"}\n",
    "{\"timestamp\":\"2024-04-06 10:14:00\",\"word\":\"Streaming\"}\n",
    "{\"timestamp\":\"2024-04-06 10:20:00\",\"word\":\"Spark\"}\n",
    "{\"timestamp\":\"2024-04-06 10:40:00\",\"word\":\"Hello\"}\n",
    "{\"timestamp\":\"2024-04-06 10:16:00\",\"word\":\"Hadoop\"}\n",
    "{\"timestamp\":\"2024-04-06 10:48:00\",\"word\":\"Streaming\"}\n",
    "```\n",
    "\n",
    "Vom obtine in loguri:\n",
    "\n",
    "```bash\n",
    "-------------------------------------------\n",
    "Batch: 1\n",
    "-------------------------------------------\n",
    "+-------------------+-------------------+---------+-----+\n",
    "|              start|                end|     word|count|\n",
    "+-------------------+-------------------+---------+-----+\n",
    "|2024-04-06 10:00:00|2024-04-06 10:15:00|Streaming|    1|\n",
    "+-------------------+-------------------+---------+-----+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 2\n",
    "-------------------------------------------\n",
    "+-------------------+-------------------+---------+-----+\n",
    "|              start|                end|     word|count|\n",
    "+-------------------+-------------------+---------+-----+\n",
    "|2024-04-06 10:00:00|2024-04-06 10:15:00|Streaming|    2|\n",
    "+-------------------+-------------------+---------+-----+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 3\n",
    "-------------------------------------------\n",
    "+-------------------+-------------------+-----+-----+\n",
    "|              start|                end| word|count|\n",
    "+-------------------+-------------------+-----+-----+\n",
    "|2024-04-06 10:15:00|2024-04-06 10:30:00|Spark|    1|\n",
    "+-------------------+-------------------+-----+-----+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 4\n",
    "-------------------------------------------\n",
    "+-------------------+-------------------+-----+-----+\n",
    "|              start|                end| word|count|\n",
    "+-------------------+-------------------+-----+-----+\n",
    "|2024-04-06 10:30:00|2024-04-06 10:45:00|Hello|    1|\n",
    "+-------------------+-------------------+-----+-----+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 5\n",
    "-------------------------------------------\n",
    "+-------------------+-------------------+------+-----+\n",
    "|              start|                end|  word|count|\n",
    "+-------------------+-------------------+------+-----+\n",
    "|2024-04-06 10:15:00|2024-04-06 10:30:00|Hadoop|    1|\n",
    "+-------------------+-------------------+------+-----+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 6\n",
    "-------------------------------------------\n",
    "+-------------------+-------------------+---------+-----+\n",
    "|              start|                end|     word|count|\n",
    "+-------------------+-------------------+---------+-----+\n",
    "|2024-04-06 10:45:00|2024-04-06 11:00:00|Streaming|    1|\n",
    "+-------------------+-------------------+---------+-----+\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observam ca desi penultima intrare are ora `10:16:00`, iar cea dinaintea ei are ora `10:40:00`, in cadrul Batch 5 Spark actualizeaza corect a doua fereastra, chiar daca aceasta este in 'urma'.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:52:31.974299Z",
     "start_time": "2024-05-07T13:52:31.908985Z"
    }
   },
   "source": [
    "thumbling_query.stop()"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:52:50.163850Z",
     "start_time": "2024-05-07T13:52:50.108674Z"
    }
   },
   "source": [
    "# Sliding window\n",
    "read_df = (\n",
    "    spark.readStream\n",
    "    .format(\"socket\")\n",
    "    .option(\"host\", \"localhost\")\n",
    "    .option(\"port\", 9999)\n",
    "    .load()\n",
    "    .withColumn(\"parsed_value\", F.from_json(\"value\", word_schema))\n",
    "    .select('parsed_value.*')\n",
    "    .filter(F.col('word').isNotNull())\n",
    "    # 15 minute window, sliding every 5 minutes, astfel fiecare event va fi in exact 3 windows (15/5=3)\n",
    "    .groupBy('word', F.window('timestamp', '15 minute', '5 minute'))\n",
    "    .agg(F.count('word').alias('count'))\n",
    ")\n",
    "read_df.printSchema()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- word: string (nullable = true)\n",
      " |-- window: struct (nullable = true)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:52:51.976297Z",
     "start_time": "2024-05-07T13:52:51.957925Z"
    }
   },
   "source": [
    "final_df = read_df.select('window.*', 'word', 'count')\n",
    "\n",
    "final_df.printSchema()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- start: timestamp (nullable = true)\n",
      " |-- end: timestamp (nullable = true)\n",
      " |-- word: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:54:00.164842Z",
     "start_time": "2024-05-07T13:53:04.574309Z"
    }
   },
   "source": [
    "sliding_query = (\n",
    "    final_df.writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"update\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "sliding_query.awaitTermination()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[41], line 8\u001B[0m\n\u001B[1;32m      1\u001B[0m sliding_query \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m      2\u001B[0m     final_df\u001B[38;5;241m.\u001B[39mwriteStream\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconsole\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mupdate\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m      6\u001B[0m )\n\u001B[0;32m----> 8\u001B[0m sliding_query\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:221\u001B[0m, in \u001B[0;36mStreamingQuery.awaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination(\u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m))\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 221\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1314\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1036\u001B[0m connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_connection()\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1038\u001B[0m     response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1039\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m binary:\n\u001B[1;32m   1040\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection_guard(connection)\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001B[0m, in \u001B[0;36mClientServerConnection.send_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    510\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 511\u001B[0m         answer \u001B[38;5;241m=\u001B[39m smart_decode(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream\u001B[38;5;241m.\u001B[39mreadline()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m    512\u001B[0m         logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnswer received: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(answer))\n\u001B[1;32m    513\u001B[0m         \u001B[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001B[39;00m\n\u001B[1;32m    514\u001B[0m         \u001B[38;5;66;03m# answer before the socket raises an error.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/socket.py:706\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    704\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 706\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39mrecv_into(b)\n\u001B[1;32m    707\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    708\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daca introducem inputurile de mai devreme putem observa ca fiecare element este in 3 ferestre, deoarece acestea se suprapun, iar lungimea unei fersetre este de 15 minute si slidingul este de 5 minute, iar 15/5 = 3. Astfel outputul va fi:\n",
    "\n",
    "```bash\n",
    "Batch: 0\n",
    "-------------------------------------------\n",
    "+-----+---+----+-----+\n",
    "|start|end|word|count|\n",
    "+-----+---+----+-----+\n",
    "+-----+---+----+-----+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 1\n",
    "-------------------------------------------\n",
    "+-------------------+-------------------+---------+-----+\n",
    "|              start|                end|     word|count|\n",
    "+-------------------+-------------------+---------+-----+\n",
    "|2024-04-06 09:55:00|2024-04-06 10:10:00|Streaming|    1|\n",
    "|2024-04-06 10:05:00|2024-04-06 10:20:00|Streaming|    1|\n",
    "|2024-04-06 10:00:00|2024-04-06 10:15:00|Streaming|    1|\n",
    "+-------------------+-------------------+---------+-----+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 2\n",
    "-------------------------------------------\n",
    "+-------------------+-------------------+---------+-----+\n",
    "|              start|                end|     word|count|\n",
    "+-------------------+-------------------+---------+-----+\n",
    "|2024-04-06 10:05:00|2024-04-06 10:20:00|Streaming|    2|\n",
    "|2024-04-06 10:10:00|2024-04-06 10:25:00|Streaming|    1|\n",
    "|2024-04-06 10:00:00|2024-04-06 10:15:00|Streaming|    2|\n",
    "+-------------------+-------------------+---------+-----+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 3\n",
    "-------------------------------------------\n",
    "+-------------------+-------------------+-----+-----+\n",
    "|              start|                end| word|count|\n",
    "+-------------------+-------------------+-----+-----+\n",
    "|2024-04-06 10:15:00|2024-04-06 10:30:00|Spark|    1|\n",
    "|2024-04-06 10:20:00|2024-04-06 10:35:00|Spark|    1|\n",
    "|2024-04-06 10:10:00|2024-04-06 10:25:00|Spark|    1|\n",
    "+-------------------+-------------------+-----+-----+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 4\n",
    "-------------------------------------------\n",
    "+-------------------+-------------------+-----+-----+\n",
    "|              start|                end| word|count|\n",
    "+-------------------+-------------------+-----+-----+\n",
    "|2024-04-06 10:30:00|2024-04-06 10:45:00|Hello|    1|\n",
    "|2024-04-06 10:35:00|2024-04-06 10:50:00|Hello|    1|\n",
    "|2024-04-06 10:40:00|2024-04-06 10:55:00|Hello|    1|\n",
    "+-------------------+-------------------+-----+-----+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 5\n",
    "-------------------------------------------\n",
    "+-------------------+-------------------+------+-----+\n",
    "|              start|                end|  word|count|\n",
    "+-------------------+-------------------+------+-----+\n",
    "|2024-04-06 10:15:00|2024-04-06 10:30:00|Hadoop|    1|\n",
    "|2024-04-06 10:10:00|2024-04-06 10:25:00|Hadoop|    1|\n",
    "|2024-04-06 10:05:00|2024-04-06 10:20:00|Hadoop|    1|\n",
    "+-------------------+-------------------+------+-----+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 6\n",
    "-------------------------------------------\n",
    "+-------------------+-------------------+---------+-----+\n",
    "|              start|                end|     word|count|\n",
    "+-------------------+-------------------+---------+-----+\n",
    "|2024-04-06 10:40:00|2024-04-06 10:55:00|Streaming|    1|\n",
    "|2024-04-06 10:35:00|2024-04-06 10:50:00|Streaming|    1|\n",
    "|2024-04-06 10:45:00|2024-04-06 11:00:00|Streaming|    1|\n",
    "+-------------------+-------------------+---------+-----+\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:54:04.990533Z",
     "start_time": "2024-05-07T13:54:04.923480Z"
    }
   },
   "source": [
    "sliding_query.stop()"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:55:16.910612Z",
     "start_time": "2024-05-07T13:55:16.840278Z"
    }
   },
   "source": [
    "# Watermarking\n",
    "\n",
    "read_df = (\n",
    "    spark.readStream\n",
    "    .format(\"socket\")\n",
    "    .option(\"host\", \"localhost\")\n",
    "    .option(\"port\", 9999)\n",
    "    .load()\n",
    "    .withColumn(\"parsed_value\", F.from_json(\"value\", word_schema))\n",
    "    .select('parsed_value.*')\n",
    "    .filter(F.col('word').isNotNull())\n",
    "    # 30 minute watermark, este important sa se apeleze withWatermark inainte de groupBy, iar coloana dupa care se face windowing si cea a watermarkului trebuie sa fie aceeasi\n",
    "    .withWatermark(\"timestamp\", \"30 minutes\")\n",
    "    .groupBy('word', F.window('timestamp', '15 minute'))\n",
    "    .agg(F.count('word').alias('count'))\n",
    ")\n",
    "read_df.printSchema()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- word: string (nullable = true)\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T13:55:19.496634Z",
     "start_time": "2024-05-07T13:55:19.484127Z"
    }
   },
   "source": [
    "final_df = read_df.select('window.*', 'word', 'count')\n",
    "\n",
    "final_df.printSchema()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- start: timestamp (nullable = true)\n",
      " |-- end: timestamp (nullable = true)\n",
      " |-- word: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T14:00:34.710775Z",
     "start_time": "2024-05-07T13:55:53.112657Z"
    }
   },
   "source": [
    "waterMark_query = (\n",
    "    final_df.writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"update\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "waterMark_query.awaitTermination()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[46], line 8\u001B[0m\n\u001B[1;32m      1\u001B[0m waterMark_query \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m      2\u001B[0m     final_df\u001B[38;5;241m.\u001B[39mwriteStream\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconsole\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mupdate\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m      6\u001B[0m )\n\u001B[0;32m----> 8\u001B[0m waterMark_query\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:221\u001B[0m, in \u001B[0;36mStreamingQuery.awaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination(\u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m))\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 221\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1314\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1036\u001B[0m connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_connection()\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1038\u001B[0m     response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1039\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m binary:\n\u001B[1;32m   1040\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection_guard(connection)\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001B[0m, in \u001B[0;36mClientServerConnection.send_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    510\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 511\u001B[0m         answer \u001B[38;5;241m=\u001B[39m smart_decode(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream\u001B[38;5;241m.\u001B[39mreadline()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m    512\u001B[0m         logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnswer received: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(answer))\n\u001B[1;32m    513\u001B[0m         \u001B[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001B[39;00m\n\u001B[1;32m    514\u001B[0m         \u001B[38;5;66;03m# answer before the socket raises an error.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/socket.py:706\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    704\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 706\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39mrecv_into(b)\n\u001B[1;32m    707\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    708\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Atentie, folosind watermaking, in consola pot aparea batchuri goale, lipsite de sens.**_\n",
    "\n",
    "Watermark Boundry = Max Event Time - Watermark Duration. Daca o fereastra are capatul superior mai mic decat acesta boundry ea va fi stearsa automat de catre spark.\n",
    "\n",
    "Punand in socket urmatoarele date de intrare:\n",
    "\n",
    "```bash\n",
    "{\"timestamp\":\"2024-04-06 10:05:00\",\"word\":\"Streaming\"}\n",
    "{\"timestamp\":\"2024-04-06 10:40:00\",\"word\":\"Hello\"}\n",
    "{\"timestamp\":\"2024-04-06 10:20:00\",\"word\":\"Spark\"}\n",
    "{\"timestamp\":\"2024-04-06 10:48:00\",\"word\":\"Streaming\"}\n",
    "{\"timestamp\":\"2024-04-06 10:14:00\",\"word\":\"Streaming\"}\n",
    "{\"timestamp\":\"2024-04-06 10:16:00\",\"word\":\"Hadoop}\n",
    "```\n",
    "\n",
    "Vom obtine in loguri:\n",
    "\n",
    "Batch: 0\n",
    "\n",
    "| start | end | word | count |\n",
    "| ----- | --- | ---- | ----- |\n",
    "|       |     |      |       |\n",
    "\n",
    "Batch: 1\n",
    "\n",
    "| start               | end                 | word      | count |\n",
    "| ------------------- | ------------------- | --------- | ----- |\n",
    "| 2024-04-06 10:00:00 | 2024-04-06 10:15:00 | Streaming | 1     |\n",
    "\n",
    "WaterMark Boundry = 10:05 - 30 minutes = 9:35\n",
    "\n",
    "Batch: 2\n",
    "\n",
    "| start               | end                 | word  | count |\n",
    "| ------------------- | ------------------- | ----- | ----- |\n",
    "| 2024-04-06 10:30:00 | 2024-04-06 10:45:00 | Hello | 1     |\n",
    "\n",
    "WaterMark Boundry = 10:40 - 30 minutes = 10:10\n",
    "\n",
    "Batch: 3\n",
    "\n",
    "| start               | end                 | word  | count |\n",
    "| ------------------- | ------------------- | ----- | ----- |\n",
    "| 2024-04-06 10:15:00 | 2024-04-06 10:30:00 | Spark | 1     |\n",
    "\n",
    "WaterMark Boundry = 10:40 - 30 minutes = 10:10 . Ramane neschimbat, deoarece eventul introdus nu este cel mai tarziu.\n",
    "\n",
    "Batch: 4\n",
    "\n",
    "| start               | end                 | word      | count |\n",
    "| ------------------- | ------------------- | --------- | ----- |\n",
    "| 2024-04-06 10:45:00 | 2024-04-06 11:00:00 | Streaming | 1     |\n",
    "\n",
    "WaterMark Boundry = 10:48 - 30 minutes = 10:18. In acest moment prima fereastra cea de la 10:00-10:15 va fi stearsa doarece capatul superior este mai mic decat boundry.\n",
    "\n",
    "Evenimentul `{\"timestamp\":\"2024-04-06 10:14:00\",\"word\":\"Streaming\"}` nu a fost luat in considerare deoarece a ajuns prea tarziu, iar fereastra a fost stearsa.\n",
    "\n",
    "Batch: 5\n",
    "\n",
    "| start               | end                 | word   | count |\n",
    "| ------------------- | ------------------- | ------ | ----- |\n",
    "| 2024-04-06 10:15:00 | 2024-04-06 10:30:00 | Hadoop | 1     |\n",
    "\n",
    "WaterMark Boundry = 10:40 - 30 minutes = 10:18 . Ramane neschimbat, deoarece eventul introdus nu este cel mai tarziu.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T14:00:39.802953Z",
     "start_time": "2024-05-07T14:00:39.710290Z"
    }
   },
   "source": [
    "waterMark_query.stop()"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T14:01:59.281543Z",
     "start_time": "2024-05-07T14:00:57.185270Z"
    }
   },
   "source": [
    "waterMarkAppend_query = (\n",
    "    final_df.writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "waterMarkAppend_query.awaitTermination()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[49], line 8\u001B[0m\n\u001B[1;32m      1\u001B[0m waterMarkAppend_query \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m      2\u001B[0m     final_df\u001B[38;5;241m.\u001B[39mwriteStream\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconsole\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m      6\u001B[0m )\n\u001B[0;32m----> 8\u001B[0m waterMarkAppend_query\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:221\u001B[0m, in \u001B[0;36mStreamingQuery.awaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination(\u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m))\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 221\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1314\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1036\u001B[0m connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_connection()\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1038\u001B[0m     response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1039\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m binary:\n\u001B[1;32m   1040\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection_guard(connection)\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001B[0m, in \u001B[0;36mClientServerConnection.send_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    510\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 511\u001B[0m         answer \u001B[38;5;241m=\u001B[39m smart_decode(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream\u001B[38;5;241m.\u001B[39mreadline()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m    512\u001B[0m         logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnswer received: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(answer))\n\u001B[1;32m    513\u001B[0m         \u001B[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001B[39;00m\n\u001B[1;32m    514\u001B[0m         \u001B[38;5;66;03m# answer before the socket raises an error.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/socket.py:706\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    704\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 706\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39mrecv_into(b)\n\u001B[1;32m    707\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    708\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daca rulam aceleasi inputuri de mai sus,dar avem modul de afisare setat pe `append`, vom vedea ca doar prima fereastra, doar dupa ce a fost stearsa din cache va fi printata. Acesta se datoreaza faptului ca `append` trebuie sa fie sigur ca fereastra nu va mai fi modificata inainte sa o scrie.\n",
    "\n",
    "Batch: 0\n",
    "\n",
    "| start | end | word | count |\n",
    "| ----- | --- | ---- | ----- |\n",
    "|       |     |      |       |\n",
    "\n",
    "Evenimentul de la 10:48 va modifica WaterMark Boundry = 10:48 - 30 minutes = 10:18. In acest moment prima fereastra de la 10:00 - 10:15 va fi stearsa din cache, deci nu va mai putea fi modificata, astfel ca aceasta va aparea in consola, devenind imutabila.\n",
    "\n",
    "Batch: 1\n",
    "\n",
    "| start               | end                 | word      | count |\n",
    "| ------------------- | ------------------- | --------- | ----- |\n",
    "| 2024-04-06 10:00:00 | 2024-04-06 10:15:00 | Streaming | 1     |\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T14:02:02.077225Z",
     "start_time": "2024-05-07T14:02:01.989980Z"
    }
   },
   "source": [
    "waterMarkAppend_query.stop()"
   ],
   "outputs": [],
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session Window\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T14:02:10.045739Z",
     "start_time": "2024-05-07T14:02:09.991152Z"
    }
   },
   "source": [
    "# Session Window\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "session_window = (\n",
    "    F.session_window(F.col('timestamp'),\n",
    "                     F.when(F.col('word') == \"Spark\", \"20 minutes\").otherwise(\"10 minutes\"))\n",
    ")\n",
    "\n",
    "read_df = (\n",
    "    spark.readStream\n",
    "    .format(\"socket\")\n",
    "    .option(\"host\", \"localhost\")\n",
    "    .option(\"port\", 9999)\n",
    "    .load()\n",
    "    .withColumn(\"parsed_value\", F.from_json(\"value\", word_schema))\n",
    "    .select('parsed_value.*')\n",
    "    .filter(F.col('word').isNotNull())\n",
    "    .groupBy('word', session_window)\n",
    "    .agg(F.count('word').alias('count'))\n",
    ")\n",
    "read_df.printSchema()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- word: string (nullable = true)\n",
      " |-- session_window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T14:02:22.314997Z",
     "start_time": "2024-05-07T14:02:22.302494Z"
    }
   },
   "source": [
    "final_df = read_df.select('session_window.*', 'word', 'count')\n",
    "\n",
    "final_df.printSchema()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- start: timestamp (nullable = true)\n",
      " |-- end: timestamp (nullable = true)\n",
      " |-- word: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T14:04:15.291805Z",
     "start_time": "2024-05-07T14:02:33.261614Z"
    }
   },
   "source": [
    "sessionWindow_query = (\n",
    "    final_df.writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"complete\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "sessionWindow_query.awaitTermination()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[54], line 8\u001B[0m\n\u001B[1;32m      1\u001B[0m sessionWindow_query \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m      2\u001B[0m     final_df\u001B[38;5;241m.\u001B[39mwriteStream\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconsole\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcomplete\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m      6\u001B[0m )\n\u001B[0;32m----> 8\u001B[0m sessionWindow_query\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:221\u001B[0m, in \u001B[0;36mStreamingQuery.awaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination(\u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m))\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 221\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1314\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1036\u001B[0m connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_connection()\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1038\u001B[0m     response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1039\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m binary:\n\u001B[1;32m   1040\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection_guard(connection)\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001B[0m, in \u001B[0;36mClientServerConnection.send_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    510\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 511\u001B[0m         answer \u001B[38;5;241m=\u001B[39m smart_decode(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream\u001B[38;5;241m.\u001B[39mreadline()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m    512\u001B[0m         logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnswer received: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(answer))\n\u001B[1;32m    513\u001B[0m         \u001B[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001B[39;00m\n\u001B[1;32m    514\u001B[0m         \u001B[38;5;66;03m# answer before the socket raises an error.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/socket.py:706\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    704\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 706\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39mrecv_into(b)\n\u001B[1;32m    707\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    708\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daca in socket vom pune ca input:\n",
    "\n",
    "```bash\n",
    "{ \"timestamp\" : \"2024-04-06 10:05:00\",\"word\":\"Streaming\" }\n",
    "{ \"timestamp\" : \"2024-04-06 10:14:00\",\"word\":\"Streaming\" }\n",
    "{ \"timestamp\" : \"2024-04-06 10:25:00\",\"word\":\"Spark\" }\n",
    "{ \"timestamp\" : \"2024-04-06 10:44:00\",\"word\":\"Streaming\" }\n",
    "{ \"timestamp\" : \"2024-04-06 10:44:00\",\"word\":\"Streaming\" }\n",
    "{ \"timestamp\" : \"2024-04-06 10:44:00\", \"word\":\"Spark\" }\n",
    "```\n",
    "\n",
    "Avand modul the afiasare `complete` vom avea outputurile:\n",
    "\n",
    "```bash\n",
    "Batch: 0\n",
    "-------------------------------------------\n",
    "+-----+---+----+-----+\n",
    "|start|end|word|count|\n",
    "+-----+---+----+-----+\n",
    "+-----+---+----+-----+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 1\n",
    "-------------------------------------------\n",
    "+-------------------+-------------------+---------+-----+\n",
    "|              start|                end|     word|count|\n",
    "+-------------------+-------------------+---------+-----+\n",
    "|2024-04-06 10:05:00|2024-04-06 10:15:00|Streaming|    1|\n",
    "+-------------------+-------------------+---------+-----+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 2\n",
    "-------------------------------------------\n",
    "+-------------------+-------------------+---------+-----+\n",
    "|              start|                end|     word|count|\n",
    "+-------------------+-------------------+---------+-----+\n",
    "|2024-04-06 10:05:00|2024-04-06 10:24:00|Streaming|    2|\n",
    "+-------------------+-------------------+---------+-----+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 3\n",
    "-------------------------------------------\n",
    "+-------------------+-------------------+---------+-----+\n",
    "|              start|                end|     word|count|\n",
    "+-------------------+-------------------+---------+-----+\n",
    "|2024-04-06 10:05:00|2024-04-06 10:24:00|Streaming|    2|\n",
    "|2024-04-06 10:25:00|2024-04-06 10:45:00|    Spark|    1|\n",
    "+-------------------+-------------------+---------+-----+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 4\n",
    "-------------------------------------------\n",
    "+-------------------+-------------------+---------+-----+\n",
    "|              start|                end|     word|count|\n",
    "+-------------------+-------------------+---------+-----+\n",
    "|2024-04-06 10:05:00|2024-04-06 10:24:00|Streaming|    2|\n",
    "|2024-04-06 10:44:00|2024-04-06 10:54:00|Streaming|    1|\n",
    "|2024-04-06 10:25:00|2024-04-06 10:45:00|    Spark|    1|\n",
    "+-------------------+-------------------+---------+-----+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 5\n",
    "-------------------------------------------\n",
    "+-------------------+-------------------+---------+-----+\n",
    "|              start|                end|     word|count|\n",
    "+-------------------+-------------------+---------+-----+\n",
    "|2024-04-06 10:05:00|2024-04-06 10:24:00|Streaming|    2|\n",
    "|2024-04-06 10:44:00|2024-04-06 10:54:00|Streaming|    2|\n",
    "|2024-04-06 10:25:00|2024-04-06 10:45:00|    Spark|    1|\n",
    "+-------------------+-------------------+---------+-----+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 6\n",
    "-------------------------------------------\n",
    "+-------------------+-------------------+---------+-----+\n",
    "|              start|                end|     word|count|\n",
    "+-------------------+-------------------+---------+-----+\n",
    "|2024-04-06 10:05:00|2024-04-06 10:24:00|Streaming|    2|\n",
    "|2024-04-06 10:44:00|2024-04-06 10:54:00|Streaming|    2|\n",
    "|2024-04-06 10:25:00|2024-04-06 11:04:00|    Spark|    2|\n",
    "+-------------------+-------------------+---------+-----+\n",
    "```\n",
    "\n",
    "Astfel, observam ca pt cuvantul `Spark` avem un window de 20 de minute, iar pentru cuvantul `Streaming` (orice cuvant diferit de Spark) avem un window de 10 minute.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T14:04:20.287265Z",
     "start_time": "2024-05-07T14:04:20.218160Z"
    }
   },
   "source": [
    "sessionWindow_query.stop()"
   ],
   "outputs": [],
   "execution_count": 55
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing To Files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrierea streamului in fisiere este foarte asemanatoare cu cea a unui DataFrame static. Metoda `writeStream` este asemanatoarea cu cea a metodei `write` pentru DataFrame-uri statice. Diferenta este ca metoda `writeStream` returneaza un obiect `DataStreamWriter` care are metode pentru configurarea streamului. Pentru scrierea in fisiere este obligatoriu sa avem un checkpoint directory, astfel incat Spark sa poata asigura consecventa datelor si unicitatea lor chiar si in cazul unei probleme. Modul de scriere in fisiere este by default cel `append`. In continuare, vom reveni la datele device pe care le vom citi din fisere json si le vom scrie in format csv.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T14:08:23.334232Z",
     "start_time": "2024-05-07T14:08:23.328897Z"
    }
   },
   "source": [
    "sourceArchiveDirJson = data_path+'/output/archive/devices/json'\n",
    "sourceArchiveDirJson"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/referat/output/archive/devices/json'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T14:08:42.708633Z",
     "start_time": "2024-05-07T14:08:42.640460Z"
    }
   },
   "source": [
    "\n",
    "read_df = spark.readStream\\\n",
    "    .option(\"sourceArchiveDir\", sourceArchiveDirJson)\\\n",
    "    .json(data_path+'/input/devices/', multiLine=True, schema=json_schema)\n",
    "\n",
    "\n",
    "final_df = create_final_df(read_df)\n",
    "\n",
    "\n",
    "final_df.printSchema()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventOffset: integer (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- eventTime: timestamp (nullable = true)\n",
      " |-- deviceId: string (nullable = true)\n",
      " |-- measure: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- temperature: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T14:10:21.566888Z",
     "start_time": "2024-05-07T14:09:53.312242Z"
    }
   },
   "source": [
    "query = (\n",
    "    final_df.writeStream\n",
    "    .format(\"csv\")\n",
    "    .option(\"cleanSource\", \"archive\")\n",
    "    .option(\"checkpointLocation\", data_path+'/checkPointDir/devices/json')\n",
    "    .option(\"path\", data_path+'/output/devices/')\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[64], line 10\u001B[0m\n\u001B[1;32m      1\u001B[0m query \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m      2\u001B[0m     final_df\u001B[38;5;241m.\u001B[39mwriteStream\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m      8\u001B[0m )\n\u001B[0;32m---> 10\u001B[0m query\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:221\u001B[0m, in \u001B[0;36mStreamingQuery.awaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination(\u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m))\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 221\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1314\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1036\u001B[0m connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_connection()\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1038\u001B[0m     response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1039\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m binary:\n\u001B[1;32m   1040\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection_guard(connection)\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001B[0m, in \u001B[0;36mClientServerConnection.send_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    510\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 511\u001B[0m         answer \u001B[38;5;241m=\u001B[39m smart_decode(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream\u001B[38;5;241m.\u001B[39mreadline()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m    512\u001B[0m         logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnswer received: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(answer))\n\u001B[1;32m    513\u001B[0m         \u001B[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001B[39;00m\n\u001B[1;32m    514\u001B[0m         \u001B[38;5;66;03m# answer before the socket raises an error.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/socket.py:706\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    704\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 706\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39mrecv_into(b)\n\u001B[1;32m    707\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    708\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daca punem fisirele `device_01.json`, `device_02.json` si `device_03.json` pe rand vom observa ca in directorul de output apar pentru fiecare fisier doua fisiere, de fapt apar 2 \\* (nr partitii), dar in cazul nostru avem doar o partitie. Primul fisier este pentru metadate, iar ce-l de-al doilea este output-ul efectiv. Daca incercam sa recitim un fisier deja citit vom observa ca in output nu se schimba nimic, deoarece Spark recunoaste numele si stie ca acel fisier a fost deja citit, astfel pastrand unicitatea outputului. Configurarea fisierului se face cu ajutorul `checkpointLocation`. Este recomandat sa nu se atinga acest fisier, deoarece Spark nu mai poate garanta consitenta datelor.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T14:10:33.767429Z",
     "start_time": "2024-05-07T14:10:33.762063Z"
    }
   },
   "source": [
    "query.stop()"
   ],
   "outputs": [],
   "execution_count": 65
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triggers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optiunea de trigger defineste cand vor fi procesate datele primite.\n",
    "\n",
    "In Spark Structured Streaming sunt 5 tipruri de triggeri:\n",
    "\n",
    "1. _Defaukt_: Procesarea este declansata de fiecare data cand un nou batch de date este disponibil.\n",
    "1. _Fixed interval micro-batches_: Procesarea este declansata la intervale fixe de timp.\n",
    "   - Daca micro-batch-ul anterior a fost procesat mai devreme, atunci sistemul asteapta pana cand intervalul de timp este atins.\n",
    "   - Daca micro-batch-ul anterior a fost procesat mai tarziu, atunci sistemul va procesa micro-batch-ul curent imediat.\n",
    "   - Daca nu sunt date noi, atunci niciun nou micro-batch nu va fi inceput.\n",
    "   ```python\n",
    "   trigger(processingTime='2 seconds')\n",
    "   ```\n",
    "1. _One-time micro-batch_ (deprecated): Procesarea este declansata o singura data, dupa care se opreste automat.\n",
    "   ```python\n",
    "   trigger(once=True)\n",
    "   ```\n",
    "1. _Available -now micro-batch_: Este la fel ca One-time micro-batch, dar procesarea datelor nu se realizeaza intr-un singur batch, ci in mai multe.\n",
    "   ```python\n",
    "   trigger(availableNow=True)\n",
    "   ```\n",
    "1. _Continuous with fixed checkpoint interval_: Procesarea este declansata la intervale fixe de timp, dar nu se bazeaza pe micro-batch-uri. In schimb, datele sunt procesate in mod continuu, fara a fi grupate in micro-batch-uri. (**Nu accepta agregari!**)\n",
    "   ```python\n",
    "   trigger(continuous='1 second')\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T14:13:09.833152Z",
     "start_time": "2024-05-07T14:12:23.779222Z"
    }
   },
   "source": [
    "# Exemplu pentru processing time 1 minute\n",
    "\n",
    "streaming_df = (\n",
    "    spark.readStream\n",
    "    .format(\"socket\")\n",
    "    .option(\"host\", \"localhost\")\n",
    "    .option(\"port\", 9999)\n",
    "    .load()\n",
    "\n",
    ")\n",
    "\n",
    "procQuery = (\n",
    "    streaming_df.writeStream\n",
    "    .format(\"console\")\n",
    "    .trigger(processingTime='1 minute')\n",
    "    .start()\n",
    ")\n",
    "\n",
    "procQuery.awaitTermination()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[66], line 19\u001B[0m\n\u001B[1;32m      3\u001B[0m streaming_df \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m      4\u001B[0m     spark\u001B[38;5;241m.\u001B[39mreadStream\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msocket\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      9\u001B[0m \n\u001B[1;32m     10\u001B[0m )\n\u001B[1;32m     12\u001B[0m procQuery \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     13\u001B[0m     streaming_df\u001B[38;5;241m.\u001B[39mwriteStream\n\u001B[1;32m     14\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconsole\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;241m.\u001B[39mtrigger(processingTime\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m1 minute\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m     17\u001B[0m )\n\u001B[0;32m---> 19\u001B[0m procQuery\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:221\u001B[0m, in \u001B[0;36mStreamingQuery.awaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination(\u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m))\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 221\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1314\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1036\u001B[0m connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_connection()\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1038\u001B[0m     response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1039\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m binary:\n\u001B[1;32m   1040\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection_guard(connection)\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001B[0m, in \u001B[0;36mClientServerConnection.send_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    510\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 511\u001B[0m         answer \u001B[38;5;241m=\u001B[39m smart_decode(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream\u001B[38;5;241m.\u001B[39mreadline()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m    512\u001B[0m         logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnswer received: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(answer))\n\u001B[1;32m    513\u001B[0m         \u001B[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001B[39;00m\n\u001B[1;32m    514\u001B[0m         \u001B[38;5;66;03m# answer before the socket raises an error.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/socket.py:706\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    704\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 706\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39mrecv_into(b)\n\u001B[1;32m    707\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    708\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T14:13:12.855374Z",
     "start_time": "2024-05-07T14:13:12.849703Z"
    }
   },
   "source": [
    "procQuery.stop()"
   ],
   "outputs": [],
   "execution_count": 67
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daca pornim streamul iar dupa aceea scriem in netcat. _Important_ : al doilea output trebuie scris dupa ce primul a fost afisat:\n",
    "\n",
    "```bash\n",
    "ana are mere\n",
    "mihai are pere\n",
    "```\n",
    "\n",
    "Vom observa ca outputurile sunt la distanta de 1 minut, mai putin intre primul si al doilea, unde spark a 'taiat' din timp deoarece acesta nu ia in considerare timpul de 'pornire'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle Partition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mai intai, un **job** este o unitate de lucru care contine o serie de **stages**. Un **stage** este o unitate de lucru care contine o serie de **taskuri**. Un **task** este o unitate de lucru care este executata pe un singur executor. Un **executor** este un proces care ruleaza pe un nod si care executa taskuri. Un **executor** poate avea mai multe **cores**. Un **core** este un thread care executa taskuri.\n",
    "\n",
    "Daca ne uitam la unul dintre joburile asocitate unei citiri pentru un input pentru datele `words` ale queriurilor anterioare:\n",
    "\n",
    "![Words Job](./images/sparkUI/wordsJob.png)\n",
    "\n",
    "Vom observa 216 taskuri: acest 216 = spark.sparkContext.defaultParallelism + shufflePartitions.\n",
    "\n",
    "- spark.sparkContext.defaultParallelism = 16, deoarece avem un master `local[*]`, iar aceasta inseamna ca-i dam lui spark voie sa foloseasca toate coreurile(virtuale) disponibile. Daca am avea `local[2]` acest numar ar fi egal cu 2. Acestea vor fi folosite pentru citirea datelor.\n",
    "- shufflePartitions = 200 by default si este numarul de partitii pe care spark le va folosi pentru operatiile de shuffle. Acest numar poate fi modificat prin `spark.sql.shuffle.partitions`. Operatia de shuffle este o operatie care implica redistribuirea datelor intre partitii, de exemplu un `groupBy` sau un `join`, in cazul nostru `groupBy`.\n",
    "\n",
    "Putem vedea aceasta distribuire intrand pe taskul respectiv si dand scroll pana la finalul paginii la sectiunea `Completed Stages`. Acolo vom vedea doua stagii:\n",
    "\n",
    "1. Unul pentru citirea datelor\n",
    "2. Unul de shuffle\n",
    "\n",
    "![Task Stages](./images/sparkUI/taskStages.png)\n",
    "\n",
    "Se poate vedea ca primul task **138** pune in Shuffle Write 99B, iar al doilea citeste acest Shuffle Write. Acest lucur se poate observa si din DAG-ul generat de spark, unde exista un Exchange.\n",
    "\n",
    "![Task DAG](./images/sparkUI/dagStages.png)\n",
    "\n",
    "Daca intram pe primul Stage vom vedea ca doar unul din cele 16 taskuri a fost folosit pentru citirea datelor, iar restul nu au facut nimic, acest lucru se datoreaza faptului ca datele vin din socket, iar Spark nu poate optimiza automat citirea.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daca am citi 3 fisiere deodata, atunci am vedea ca 3 din cele 16 taskuri vor fi instantiate pentru citire, insa tot 200 vor fi folosite pentru shuffle.\n",
    "\n",
    "Vom pune intr-un folder input pentru `words` cele trei fisier disponibile `words.txt`, `words_copy_01.txt`, `words_copy_02.txt` si le vom citi concomitent.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T14:15:59.086173Z",
     "start_time": "2024-05-07T14:15:59.081695Z"
    }
   },
   "source": [
    "data_path"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/referat'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T14:17:12.710214Z",
     "start_time": "2024-05-07T14:17:10.846904Z"
    }
   },
   "source": [
    "df = (spark.readStream\n",
    "      .text(data_path+'/input/words/')\n",
    "      .withColumn('parsedValue', F.from_json(F.col('value'), word_schema))\n",
    "      .select('parsedValue.*')\n",
    "      .filter(F.col('word').isNotNull())\n",
    "      .groupBy('word')\n",
    "      .agg(F.count('word').alias('count'))\n",
    "      )\n",
    "\n",
    "\n",
    "query = (\n",
    "    df.writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"complete\")\n",
    "    .trigger(availableNow=True)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()"
   ],
   "outputs": [],
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T14:17:14.645996Z",
     "start_time": "2024-05-07T14:17:14.643006Z"
    }
   },
   "source": [
    "query.stop()"
   ],
   "outputs": [],
   "execution_count": 72
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jobul asociat citrii celor 3 fisiere se poate observa ca are 203 taskuri nu 216, deci spark a putut optimiza citirea.\n",
    "\n",
    "![Files Read Job](./images/sparkUI/filesReadJob.png)\n",
    "\n",
    "Daca intram pe acest Job, se poate vedea clar ca optimizarea a avut loc la citire, deoarce avem 3 fisere si le citim concomitent, deci Spark ne ofera 3 taskuri pentru citire.\n",
    "\n",
    "![File Read Stages](./images/sparkUI/fileReadStages.png)\n",
    "\n",
    "Mai departe, intrand pe stageul de citire putem vedea ca fiecare task a lucrat si nu exista taskuri care nu au facut nimc.\n",
    "\n",
    "![Reading File Stage Tasks](./images/sparkUI/readingFileStageTasks.png)\n",
    "\n",
    "In schimb daca intram pe stageul de shuffle putem vedea ca majoritatea taskurilor nu sunt utile, ele nefacand ceva in shuffle.\n",
    "\n",
    "In general trebuie sa avem grija cand schimbam configurarea `spark.sql.shuffle.partitions`, deoarece putem incetini seminficativ partitionarea si chiar sa ramanem fara memorie in executori daca numarul este prea mic pentru datele transmise. Insa in cazul nostru stim ca numarul de date este mic (chiar infim pentru spark), asa ca vom incerca sa setam numarul de partitii la 8.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T14:18:47.718275Z",
     "start_time": "2024-05-07T14:18:47.714295Z"
    }
   },
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")"
   ],
   "outputs": [],
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T14:19:02.417583Z",
     "start_time": "2024-05-07T14:19:02.027356Z"
    }
   },
   "source": [
    "df = (spark.readStream\n",
    "      .text(data_path+'/input/words/')\n",
    "      .withColumn('parsedValue', F.from_json(F.col('value'), word_schema))\n",
    "      .select('parsedValue.*')\n",
    "      .filter(F.col('word').isNotNull())\n",
    "      .groupBy('word')\n",
    "      .agg(F.count('word').alias('count'))\n",
    "      )\n",
    "\n",
    "\n",
    "query = (\n",
    "    df.writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"complete\")\n",
    "    .trigger(availableNow=True)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "query.stop()"
   ],
   "outputs": [],
   "execution_count": 74
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daca rulam aceeasi citire a celor 3 fisiere putem observa o diferenta considerabile in timpul de rulare. Timpul a scazut de la **2s** la **0.1s**.\n",
    "Numarul de taskuri utilizat va fi 11 = 3 citire + 8 shuffle.\n",
    "\n",
    "![Files Read Job Shuffle](./images/sparkUI/filesReadJobShuffle.png)\n",
    "\n",
    "Intrand in stagiul de shuffle, observam ca 3 din cele 8 taskuri sunt utilizate, ceea ce reflecta modul in care Spark optimizeaza gestionarea resurselor. Acest numar de **3** nu este aleatoriu. Spark isi adapteaza comportamentul de procesare a datelor in functie de volumul si structura acestora:\n",
    "\n",
    "- **Partitionare Adaptiva**: Initial, datele sunt impartite in 3 partitii in faza de citire. Spark foloseste aceasta informatie pentru a ajusta numarul de taskuri active in etapa de shuffle. Prin evaluarea dimensiunii si distributiei datelor in aceste partitii, Spark determina ca 3 taskuri sunt suficiente pentru a procesa eficient datele, evitand astfel risipa de resurse computationale.\n",
    "\n",
    "- **Reducerea Overhead-ului**: Utilizand un numar mai mic de taskuri decat numarul maxim disponibil, Spark reduce overhead-ul asociat cu gestionarea unui numar mare de taskuri si comunicatiile de retea inter-node care pot deveni costisitoare, mai ales daca datele sunt deja bine partitionate.\n",
    "\n",
    "- **Optimizarea Performantei**: Aceasta abordare nu doar ca optimizeaza utilizarea resurselor dar si imbunatateste performanta generala a jobului de Spark. Taskurile sunt dimensionate adecvat pentru a balansa incarcarea de lucru si a minimiza timpul de asteptare intre taskuri.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T14:19:49.529601Z",
     "start_time": "2024-05-07T14:19:49.364442Z"
    }
   },
   "source": [
    "spark.stop()"
   ],
   "outputs": [],
   "execution_count": 75
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apache Kafka** este o platforma distribuita de streaming de evenimente/mesaje care este utilizata in construirea 'pipelineurilor' de streaming pentru aplicatii. Este conceput pentru gestionarea volumelor mari de date intr-o maniera scalabila si rezistenta la erori, facandu-l ideal pentru programarea reactiva si arhitecturi de tipul event driven. La baza Kafka este un sistem de mesaje distribuite avand arhitectura _publish-subcribe_, asemanator programarii reactive. Datele/Mesajele din Kafka sunt trimise de catre _producer_ intr-un _topic_, iar apoi sunt receptionate de catre _consumerii_ abonati la topicurile respective, permitand procesarea paralela a datelor si replicarea acestora pe mai multi _brokeri_.\n",
    "\n",
    "In cadrul Kafka nu trebuie sa ne gandim prima data la date, ci la evenimentele asociate cu acestea. Fiecare mesaj este la urma urmei un state al unui eveniment. Datele din eveniment descriu ce s-a intamplat, cand s-a itamplat si cine a fost implicat. Kafka proceseaza evenimentele din topicuri in mod cronologic, iar spre deosebire de alte techologii de tip _message que_ precum RabbitMQ, un mesaj din Kafka nu este sters odata ce este consumat, ci este sters fie dupa o perioada de timp, fie cand memoria depaseste un anumit prag.\n",
    "\n",
    "![Kafka Cluster](https://static.javatpoint.com/tutorial/kafka/images/apache-kafka-tutorial-1.png)\n",
    "\n",
    "**Terminologie**\n",
    "Kafka este un sistem distribuit bazat pe arhitectura de tip cluster, iar comunicarea dintre clienti si servere se realizeaza prin intermediul [TCP](https://www.fortinet.com/resources/cyberglossary/tcp-ip).\n",
    "\n",
    "1. _**Brokers**_ : se refra la un server din tierul de stocare al evenimentelor din Kafka pentru una sau mai multe surse. Un cluster din Kafka este de obicei compus din mai multi brokeri. Fiecare broker dintr-un cluster este si un server bootsrap, insemnand ca daca se realizeaza conexiunea cu un server, atunci se va realiza cu toti. In special, acest tip de server are rolul de oferi clientului metadatele despre cluster. In final, un borker este doar un mod de conectare al consumatorilor de producatori.\n",
    "\n",
    "1. _**Topics**_ : modul de organizare al evenimentelor din Kafka. Un topic este ca o categorie. Un topic este un 'log' (jurnal) de evenimente. Un topic are urmatoarele caracteristici:\n",
    "\n",
    "   - Este 'append only'. Cand un nou mesaj este scris, acesta este adugat la coada.\n",
    "   - Mesajele din topic sunt imutabile.\n",
    "   - Un consumator citeste un topic de la un anumit offset pana la capat. Primul elemnt are offestul 0, al doilea il are 1 etc.\n",
    "   - Este multi-producer si multi-subscriber. Un topic poate avea zero sau mai multi produceri care scriu in el si zero sau mai multi consumeri care citesc din el.\n",
    "\n",
    "   Spre deosebire de sistemele de cozi de mesaje evenimentele dintr-un topic pot fi citite de mai multe ori si nu sunt sterse dupa prima consumare. In schimb, ele sunt sterse fie cand au atins un threshold de timp sau cand topicul a atins o anumita dimensiune.\n",
    "\n",
    "1. _**Producers**_ : sunt clientii care scriu mesaje in Kafka. Un producer scrie mesaje catre anumite topicuri. De asemnea, acestia au posibilitatea de a partitiona scrierea.\n",
    "\n",
    "1. _**Consumers**_ : sunt clientii care citestc mesajele din Kafka. Acestia sunt responsabili de a-si acutaliza singuri offseturile. Un consumator poate sa treaca si la un offset din trecut, daca este nevoie, atat timp cat datele mai sunt in acel topic. Marele avantaj al lui Kafka este ca fiecare consumator este independent.\n",
    "\n",
    "1. _**Partitions**_ : datele sau mesajele sunt impartite in parti de mici dimensiuni. Fiecare partitie va avea un offset propriu. Datele sunt mereu scrise in ordine secventiala. Nu se poate garanta in ce partitie va fi scrisa o bucata dintr-un mesaj.\n",
    "\n",
    "1. _**Zookeper**_ : are rolul de a stoca informatii despre cluster si despre consumers. Acesta gestioneaza brokerii prin mentinarea unei liste, de asemnea Zookeper este responsabil si pentru alegerea unui lider al partitiilor. Daca apar modificari precum schimbarea brokerilor, introducerea de topicuri, Zookeper notifica nodurile clusterului Kafka. Un consumator nu interactioneaza direct cu Zookeper, ci el interactioneaza cu borkerii (bootsrap server).\n",
    "\n",
    "![Kafka Cluster Arhitecture](./images/kafka/kafkaCluster.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In environementul nostru local:\n",
    "\n",
    "```yml\n",
    "bd-zookeeper:\n",
    "  image: confluentinc/cp-zookeeper:latest\n",
    "  container_name: bd-zookeeper\n",
    "  ports:\n",
    "    - 2181:2181\n",
    "  environment:\n",
    "    ZOOKEEPER_CLIENT_PORT: 2181\n",
    "    ZOOKEEPER_TICK_TIME: 2000\n",
    "\n",
    "bd-kafka:\n",
    "  image: confluentinc/cp-kafka:latest\n",
    "  container_name: bd-kafka\n",
    "  depends_on:\n",
    "    - bd-zookeeper\n",
    "  ports:\n",
    "    - 9092:9092\n",
    "  volumes:\n",
    "    - streaming_data:/data:rw\n",
    "  environment:\n",
    "    KAFKA_BROKER_ID: 1\n",
    "    KAFKA_ZOOKEEPER_CONNECT: bd-zookeeper:2181\n",
    "    KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://bd-kafka:29092,PLAINTEXT_HOST://127.0.0.1:9092\n",
    "    KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n",
    "    KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "```\n",
    "\n",
    "avem un cluster Kafka cu un singur broker.\n",
    "\n",
    "Pentru _Zookepeer_ (bd-zookeeper) avem configurararile:\n",
    "\n",
    "- Expunerea portului `2181` pentru conectarea brokerilor\n",
    "- `ZOOKEEPER_CLIENT_PORT: 2181` : are rolul de a inregistra portul (default) catre broker pentru conexiune.\n",
    "- `ZOOKEEPER_TICK_TIME: 2000` : este unitatea de timp in ms folosita pentru 'heartbeaturi' ie semnale periodice trimise intre Zookerp si borker pentru a confirma validitatea conexiunii.\n",
    "\n",
    "Pentru _Broker_ (bd-kafka) avem configurarile:\n",
    "\n",
    "- Expunerea portului `9092` pentru conectarea clientilor\n",
    "- `KAFKA_BROKER_ID: 1` : id-ul brokerului. Fiecare broker in Kafka trebuie neparat sa aiba un id unic in cluster.\n",
    "- `KAFKA_ZOOKEEPER_CONNECT: bd-zookeeper:2181` : Conectarea la Zookeepr prin portul expus de catrea acesta, anume 2181.\n",
    "- `KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://bd-kafka:29092,PLAINTEXT_HOST://127.0.0.1:9092` : specifica faptul ca modul de interactiune atat in networkul din Docker pe portul 29092 cat si in localhost din masina pe portul 9092 sa fie facut prin PLAINTEXT.\n",
    "- `KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT` : specifica faptul ca intre clienti si borkeri comunicarile nu vor fi criptate ie PLAINTEXT ramane nemodificat.\n",
    "- `KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1` : in Kafka, ca in multe sisteme distribuite precum Hadoop, datele pot fi replicate pe mai multe noduri. In cazul nostru, avem un singur broker daca am seta factorul de replicare mai mare decat 1, am replica pe aceeasi masina, ceea ce combate scopul replicarii, asadar am setat acest numar la 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hellow World Kafka\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pentru a lucra cu primul nostru topic, mai intai sa pornim clusterul, iar apoi sa ne conectam la broker prin:\n",
    "\n",
    "```bash\n",
    "docker exec -it bd-kafka bash\n",
    "```\n",
    "\n",
    "Mai intai sa vedem daca exista topicuri in cluster:\n",
    "\n",
    "```bash\n",
    "kafka-topics --list --bootstrap-server localhost:29092\n",
    "```\n",
    "\n",
    "Comanda listeaza toate topicurile disponibile pe cluster, folosim localhost:29092, deoarece suntem conectati la broker, iar acesta expune in mediul Docker portul 29092.\n",
    "Ca output vom avea:\n",
    "\n",
    "```bash\n",
    "__consumer_offsets\n",
    "```\n",
    "\n",
    "Acesta este un topic generat de Kafka folosit pentru a memora informatii despre offseturile clientilor cu privire la fiecare topic folosit de catre acestia si respectiv fiecare partitie a topicurilor folosite. Este _indicat_ sa nu se modifice acest topic.\n",
    "\n",
    "Pentru a ne crea propriul nostru topic vom folosi comanda:\n",
    "\n",
    "```bash\n",
    "kafka-topics --create --topic big-data-topic --bootstrap-server localhost:29092\n",
    "```\n",
    "\n",
    "daca totul a mers ok vom primi\n",
    "\n",
    "```bash\n",
    "Created topic big-data-topic.\n",
    "```\n",
    "\n",
    "Asadar, am creat un topic numit _big-data-topic_ in cadrul clusterului nostru.\n",
    "\n",
    "Pentru a vedea informatiile despre acest topic vom scrie:\n",
    "\n",
    "```bash\n",
    "kafka-topics --describe --topic big-data-topic --bootstrap-server localhost:29092\n",
    "```\n",
    "\n",
    "Outputul va fi:\n",
    "\n",
    "```bash\n",
    "Topic: big-data-topic   TopicId: Jz0qfweDT4KfzFuI17-Jaw PartitionCount: 1       ReplicationFactor: 1    Configs:\n",
    "      Topic: big-data-topic   Partition: 0    Leader: 1       Replicas: 1     Isr: 1\n",
    "```\n",
    "\n",
    "Cum nu am specificat numarul de partitii si numarul de replici, topicul nostru are 1 partie si 1 replica. (In cazul nostru este inutila specificarea acestora, avand doar un broker). Leader este borkerul 'principal' al partitiei, fiind responsabil de scrierea si citirea datelor din acea partitie. Daca Leaderul nu este valabil intr-un cluster, unul din ceilalti brokeri followeri ai partitiei vor fi desemnati lideri in mod automat. ISR(In Sync Replicas) este numarul de replici care respecta ultima instanta a mesajelor din topic.\n",
    "\n",
    "Daca incercam sa crem un topic cu numarul de replici mai mare decat numarul de brokeri, vom primi o eroare deoarece Kafka nu permita existenta a doua replici identice pe acelasi broker. Asadar, in cazul nostru, replication factor trebuie sa fie neparat 1, insa putem partitiona datele in cadrul brokerului:\n",
    "\n",
    "```bash\n",
    "kafka-topics --create --topic big-data-topic-2 --partitions 3 --replication-factor 1 --bootstrap-server localhost:29092\n",
    "```\n",
    "\n",
    "Comanda va crea un topic numit _big-data-topic-2_ cu 3 partitii si cu replication factor dat explicit ca 1 (chiar daca este defaultul).\n",
    "Descrierea noului topic:\n",
    "\n",
    "```bash\n",
    "kafka-topics --describe --topic big-data-topic-2 --bootstrap-server localhost:29092\n",
    "```\n",
    "\n",
    "```bash\n",
    "Topic: big-data-topic-2 TopicId: Lr_FoUwaT9qE-434Lr05Ig PartitionCount: 3       ReplicationFactor: 1    Configs:\n",
    "        Topic: big-data-topic-2 Partition: 0    Leader: 1       Replicas: 1     Isr: 1\n",
    "        Topic: big-data-topic-2 Partition: 1    Leader: 1       Replicas: 1     Isr: 1\n",
    "        Topic: big-data-topic-2 Partition: 2    Leader: 1       Replicas: 1     Isr: 1\n",
    "```\n",
    "\n",
    "Dupa cum am mai mentionat si anterior, toate replicile vor avea acelasi lider avand doar un broker. In cazul mai multor broker se va incerca o distribuire a acestora.\n",
    "Pentru a vedea fiecare offset ('lungimea') al partitilor create vom folosi:\n",
    "\n",
    "```bash\n",
    "kafka-get-offsets --topic big-data-topic-2 --bootstrap-server localhost:29092\n",
    "```\n",
    "\n",
    "Ca output vom avea:\n",
    "\n",
    "```bash\n",
    "big-data-topic-2:0:0\n",
    "big-data-topic-2:1:0\n",
    "big-data-topic-2:2:0\n",
    "```\n",
    "\n",
    "Outputul are forma <topic name>:<replica number>:<offset number>. Cum numele topicului nostru este big-data-topic-2 si avem trei replici, 0, 1 si 2, iar nu avem niciun mesaj in topic, constatam ca outputul reflecta realitatea.\n",
    "Pentru a publica mesaje in topic vom deschide un nou terminal in cadrul `bd-kafka` si vom scrie:\n",
    "\n",
    "```bash\n",
    "kafka-console-producer --topic big-data-topic-2 --bootstrap-server localhost:29092\n",
    "```\n",
    "\n",
    "Vom crea un producer. Interactiunea cu acesta este asemanatoare cu cea a unui socket Netcat. Daca introducem mesajele:\n",
    "\n",
    "```bash\n",
    "big data\n",
    "typescript should be used\n",
    "```\n",
    "\n",
    "si apoi afisam din nou offseturile, folosind celalalt terminal, vom obtine:\n",
    "\n",
    "```bash\n",
    "big-data-topic-2:0:0\n",
    "big-data-topic-2:1:0\n",
    "big-data-topic-2:2:2\n",
    "```\n",
    "\n",
    "Putem vedea ca mesajele au fost introduse in cadrul unei partitii, anume a treia, aceasta nu este o regula. (In cazul nostru nu este o intamplare deloc, deoarece avem acelasi lider al replicilor, deci ele sunt redundante, asadar Kafka insereaza doar in una din replici pentru sesiunea consumerului.)\n",
    "Pentru a crea un consumer, din nou vom crea inca un terminal conectat la containerul de kafka si vom scrie:\n",
    "\n",
    "```bash\n",
    "kafka-console-consumer --topic big-data-topic-2 --bootstrap-server localhost:29092 --partition 2 --offset earliest\n",
    "```\n",
    "\n",
    "In cadrul comenzii am specificat partitia dorita, in cazul nostru a treia, iar offsetul ca earliest (acesta poate fi de exemplu si 1 ie de la al doilea mesaj inclusiv), adica de la inceput. Daca rulam vom vedea cele doua mesaje:\n",
    "\n",
    "```bash\n",
    "big data\n",
    "typescript should be used\n",
    "```\n",
    "\n",
    "Daca dorim sa accesam mesajele noi din topic indiferent de partitie, vom rula:\n",
    "\n",
    "```bash\n",
    "kafka-console-consumer --topic big-data-topic-2 --bootstrap-server localhost:29092\n",
    "```\n",
    "\n",
    "La inceput nu vom vedea mesajele vechi intrucat nespecificand partitia nu avem un singur offset, ci avem trei diferite pentru partitii, asadar vom vedea doar mesajele nou introduse ie cele introduse dupa abonarea la topic a consumerului.\n",
    "Acum, daca in terminalul producer vom introduce mesaje acestea vor aparea 'live' in terminalul de consumer.\n",
    "\n",
    "Daca mai introducem de exemplu inca doua mesaje si reafisam offseturile, vom obtine:\n",
    "\n",
    "```bash\n",
    "big-data-topic-2:0:0\n",
    "big-data-topic-2:1:2\n",
    "big-data-topic-2:2:2\n",
    "```\n",
    "\n",
    "Vom inchide terminalele de producer si consumer si apoi vom sterge topicul folosind:\n",
    "\n",
    "```bash\n",
    "kafka-topics --delete --topic big-data-topic-2 --bootstrap-server localhost:29092\n",
    "```\n",
    "\n",
    "Daca relistam topicurile vom vedea ca _big-data-topic-2_ a disparut.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kafka With Spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading From Kafka\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pentru a conecta o aplicatie Pyspark la Kafka trebuie sa folosim un jar care contine dependentele necesare pentru a realiza aceasta conexiune. In cazul nostru vom folosi jarul `org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0` care poate fi gasit in directorul jars. Acesta contine toate dependentele necesare pentru a realiza conexiunea intre Kafka si Spark. Avem:\n",
    "\n",
    "- `org.apache.spark` : este grupul care a creat jarul\n",
    "- `spark-sql-kafka-0-10_2.12` : este numele artefactului\n",
    "- `3.5.0` : este versiunea jarului si a Spark-ului\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:30:28.973835Z",
     "start_time": "2024-05-08T12:30:17.771669Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Spark with Kafka\")\n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0')\n",
    "    .config(\"spark.sql.shuffle.partitions\", 16)\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pentru a citi datele din Kafka vom folosi metoda `readStream` a obiectului `spark` si metoda `format` pentru a specifica formatul de citire, in cazul nostru `kafka`. Pentru a specifica serverele de bootstrapping vom folosi metoda `option` si cheia `kafka.bootstrap.servers`. Pentru a specifica topicul vom folosi metoda `option` si cheia `subscribe`. In cazul nostru vom citi din topicul dorit.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:31:11.078782Z",
     "start_time": "2024-05-08T12:31:11.075731Z"
    }
   },
   "source": [
    "# adresa brokerului in networkul din Docker\n",
    "KAFKA_BOOTSTRAP_SERVER = 'bd-kafka:29092'\n",
    "TOPIC = 'bd-topic'"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:32:16.418756Z",
     "start_time": "2024-05-08T12:32:14.975630Z"
    }
   },
   "source": [
    "df = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVER)  # brokerul\n",
    "    .option(\"subscribe\", TOPIC)  # topicul\n",
    "    .option(\"startingOffsets\", \"earliest\")  # incepem de la primul mesaj\n",
    "    .load()\n",
    ")\n",
    "\n",
    "df.printSchema()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un stream citit din Kafka este un DataFrame cu urmatoarele coloane:\n",
    "\n",
    "| Column            | Type      | Description            |\n",
    "| ----------------- | --------- | ---------------------- |\n",
    "| key               | binary    | Cheia mesajului        |\n",
    "| value             | binary    | Valoarea mesajului     |\n",
    "| topic             | string    | Numele topicului       |\n",
    "| partition         | int       | Numarul partitiei      |\n",
    "| offset            | long      | Offset-ul mesajului    |\n",
    "| timestamp         | timestamp | Timestamp-ul mesajului |\n",
    "| timestampType     | int       | Tipul timestamp-ului   |\n",
    "| headers(optional) | array     | Headers ale mesajului  |\n",
    "\n",
    "Mesajul efectiv se afla in coloana `value`.\n",
    "\n",
    "Mai intai vom trimite cuvinte in Kafka folosind un producer din terminalul containerului de Kafka, iar apoi doar vom numara cuvintele din mesaje.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:33:17.860257Z",
     "start_time": "2024-05-08T12:33:17.646342Z"
    }
   },
   "source": [
    "count_df = (\n",
    "    df.selectExpr(\"CAST(value AS STRING) as word\")\n",
    "    .groupBy(\"word\")\n",
    "    .count()\n",
    ")\n",
    "count_df.printSchema()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- word: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inainte de a porni streamul trebuie sa crem topicul `bd-topic` in Kafka, iar apoi trebuie sa ne conectam la acesta folosind un producer:\n",
    "\n",
    "```bash\n",
    "kafka-topics --create --topic bd-topic --bootstrap-server localhost:9092\n",
    "```\n",
    "\n",
    "Dupa care:\n",
    "\n",
    "```bash\n",
    "kafka-console-producer --topic bd-topic --bootstrap-server localhost:9092\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:34:42.763027Z",
     "start_time": "2024-05-08T12:34:27.132789Z"
    }
   },
   "source": [
    "query = (\n",
    "    count_df.writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"complete\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 8\u001B[0m\n\u001B[1;32m      1\u001B[0m query \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m      2\u001B[0m     count_df\u001B[38;5;241m.\u001B[39mwriteStream\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconsole\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcomplete\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m      6\u001B[0m )\n\u001B[0;32m----> 8\u001B[0m query\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:221\u001B[0m, in \u001B[0;36mStreamingQuery.awaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination(\u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m))\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 221\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1314\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1036\u001B[0m connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_connection()\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1038\u001B[0m     response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1039\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m binary:\n\u001B[1;32m   1040\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection_guard(connection)\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001B[0m, in \u001B[0;36mClientServerConnection.send_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    510\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 511\u001B[0m         answer \u001B[38;5;241m=\u001B[39m smart_decode(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream\u001B[38;5;241m.\u001B[39mreadline()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m    512\u001B[0m         logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnswer received: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(answer))\n\u001B[1;32m    513\u001B[0m         \u001B[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001B[39;00m\n\u001B[1;32m    514\u001B[0m         \u001B[38;5;66;03m# answer before the socket raises an error.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/socket.py:706\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    704\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 706\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39mrecv_into(b)\n\u001B[1;32m    707\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    708\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:34:45.755580Z",
     "start_time": "2024-05-08T12:34:45.740289Z"
    }
   },
   "source": [
    "query.stop()"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vom vedea ca query-ul se comporta ca si cum am scrie in socket. Pe Spark nu-l intereseaza de unde vin datele si acesta este un avantaj al lui Spark Streaming.\n",
    "\n",
    "Daca repornim queryul vedem ca reapar mesajele din topic, acest fapt se datoreaza faptului ca Kafka nu sterge mesajele dupa ce acestea sunt consumate, ci le sterge in mod implicit in 7 zile daca nu am specificat nimic.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:34:58.029352Z",
     "start_time": "2024-05-08T12:34:55.163838Z"
    }
   },
   "source": [
    "query = (\n",
    "    count_df.writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"complete\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 8\u001B[0m\n\u001B[1;32m      1\u001B[0m query \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m      2\u001B[0m     count_df\u001B[38;5;241m.\u001B[39mwriteStream\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconsole\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcomplete\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m      6\u001B[0m )\n\u001B[0;32m----> 8\u001B[0m query\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:221\u001B[0m, in \u001B[0;36mStreamingQuery.awaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination(\u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m))\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 221\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1314\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1036\u001B[0m connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_connection()\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1038\u001B[0m     response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1039\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m binary:\n\u001B[1;32m   1040\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection_guard(connection)\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001B[0m, in \u001B[0;36mClientServerConnection.send_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    510\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 511\u001B[0m         answer \u001B[38;5;241m=\u001B[39m smart_decode(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream\u001B[38;5;241m.\u001B[39mreadline()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m    512\u001B[0m         logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnswer received: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(answer))\n\u001B[1;32m    513\u001B[0m         \u001B[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001B[39;00m\n\u001B[1;32m    514\u001B[0m         \u001B[38;5;66;03m# answer before the socket raises an error.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/socket.py:706\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    704\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 706\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39mrecv_into(b)\n\u001B[1;32m    707\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    708\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:35:00.513498Z",
     "start_time": "2024-05-08T12:35:00.503681Z"
    }
   },
   "source": [
    "query.stop()"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dupa cum am specificat si in introducere **Consumerul este responsabil de mesajele pe care le doreste**, insa folosind Spark putem utiliza checkpointing pentru a pastra offseturile si pentru a nu pierde datele in cazul unui esec.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:35:36.513745Z",
     "start_time": "2024-05-08T12:35:36.507541Z"
    }
   },
   "source": [
    "import os\n",
    "data_path = os.path.join(os.getcwd(), \"referat\")\n",
    "data_path"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/referat'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:35:41.278707Z",
     "start_time": "2024-05-08T12:35:41.274482Z"
    }
   },
   "source": [
    "kafka_bd_topic_dir = os.path.join(data_path, \"checkPointDir\", \"kafka\", \"words\")\n",
    "kafka_bd_topic_dir"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/referat/checkPointDir/kafka/words'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La prima apelare cu checkpoint dir vom vedea in continuare mesajele, insa daca oprim queryul si il repornim vom vedea ca nu mai apar mesajele, deoarece Spark a pastrat offseturile si a citit doar mesajele noi.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:36:59.884156Z",
     "start_time": "2024-05-08T12:36:37.674041Z"
    }
   },
   "source": [
    "query_check = (\n",
    "    count_df.writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"complete\")\n",
    "    .option(\"checkpointLocation\", kafka_bd_topic_dir)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query_check.awaitTermination()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 9\u001B[0m\n\u001B[1;32m      1\u001B[0m query_check \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m      2\u001B[0m     count_df\u001B[38;5;241m.\u001B[39mwriteStream\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconsole\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m      7\u001B[0m )\n\u001B[0;32m----> 9\u001B[0m query_check\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:221\u001B[0m, in \u001B[0;36mStreamingQuery.awaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination(\u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m))\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 221\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1314\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1036\u001B[0m connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_connection()\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1038\u001B[0m     response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1039\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m binary:\n\u001B[1;32m   1040\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection_guard(connection)\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001B[0m, in \u001B[0;36mClientServerConnection.send_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    510\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 511\u001B[0m         answer \u001B[38;5;241m=\u001B[39m smart_decode(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream\u001B[38;5;241m.\u001B[39mreadline()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m    512\u001B[0m         logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnswer received: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(answer))\n\u001B[1;32m    513\u001B[0m         \u001B[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001B[39;00m\n\u001B[1;32m    514\u001B[0m         \u001B[38;5;66;03m# answer before the socket raises an error.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/socket.py:706\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    704\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 706\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39mrecv_into(b)\n\u001B[1;32m    707\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    708\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:37:04.260734Z",
     "start_time": "2024-05-08T12:37:04.252300Z"
    }
   },
   "source": [
    "query_check.stop()"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repornind queryul vom vedea ca nu mai apar mesajele vechi, ci doar cele noi.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:37:22.403316Z",
     "start_time": "2024-05-08T12:37:15.558955Z"
    }
   },
   "source": [
    "query_check = (\n",
    "    count_df.writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"complete\")\n",
    "    .option(\"checkpointLocation\", kafka_bd_topic_dir)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query_check.awaitTermination()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 9\u001B[0m\n\u001B[1;32m      1\u001B[0m query_check \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m      2\u001B[0m     count_df\u001B[38;5;241m.\u001B[39mwriteStream\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconsole\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m      7\u001B[0m )\n\u001B[0;32m----> 9\u001B[0m query_check\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:221\u001B[0m, in \u001B[0;36mStreamingQuery.awaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination(\u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m))\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 221\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1314\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1036\u001B[0m connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_connection()\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1038\u001B[0m     response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1039\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m binary:\n\u001B[1;32m   1040\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection_guard(connection)\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001B[0m, in \u001B[0;36mClientServerConnection.send_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    510\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 511\u001B[0m         answer \u001B[38;5;241m=\u001B[39m smart_decode(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream\u001B[38;5;241m.\u001B[39mreadline()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m    512\u001B[0m         logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnswer received: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(answer))\n\u001B[1;32m    513\u001B[0m         \u001B[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001B[39;00m\n\u001B[1;32m    514\u001B[0m         \u001B[38;5;66;03m# answer before the socket raises an error.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/socket.py:706\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    704\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 706\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39mrecv_into(b)\n\u001B[1;32m    707\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    708\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:37:25.464661Z",
     "start_time": "2024-05-08T12:37:25.458214Z"
    }
   },
   "source": [
    "query_check.stop()"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sa crem un nou topic pentru a explora si celelalte atribute ale datelor din Kafka.\n",
    "Vom crea un nou topic `bd-topic-2` si doar vom afisa mesajele din el pentru inceput.\n",
    "\n",
    "**OBS** Pentru a folosi outputmode-ul `complete` cu Kafka trebuie sa avem o agregare intrucat se poate ajunge foare usor la un volum mare de date, iar Spark va ramane fara memorie.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:38:16.970988Z",
     "start_time": "2024-05-08T12:38:16.967935Z"
    }
   },
   "source": [
    "TOPIC_2 = 'bd-topic-2'"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:39:27.987557Z",
     "start_time": "2024-05-08T12:38:31.474783Z"
    }
   },
   "source": [
    "query = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVER)\n",
    "    .option(\"subscribe\", TOPIC_2)\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "query.awaitTermination()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 13\u001B[0m\n\u001B[1;32m      1\u001B[0m query \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m      2\u001B[0m     spark\u001B[38;5;241m.\u001B[39mreadStream\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkafka\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m     12\u001B[0m )\n\u001B[0;32m---> 13\u001B[0m query\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:221\u001B[0m, in \u001B[0;36mStreamingQuery.awaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination(\u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m))\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 221\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1314\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1036\u001B[0m connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_connection()\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1038\u001B[0m     response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1039\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m binary:\n\u001B[1;32m   1040\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection_guard(connection)\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001B[0m, in \u001B[0;36mClientServerConnection.send_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    510\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 511\u001B[0m         answer \u001B[38;5;241m=\u001B[39m smart_decode(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream\u001B[38;5;241m.\u001B[39mreadline()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m    512\u001B[0m         logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnswer received: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(answer))\n\u001B[1;32m    513\u001B[0m         \u001B[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001B[39;00m\n\u001B[1;32m    514\u001B[0m         \u001B[38;5;66;03m# answer before the socket raises an error.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/socket.py:706\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    704\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 706\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39mrecv_into(b)\n\u001B[1;32m    707\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    708\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daca in producer introducem o valoare, de exemplu `big data`, vom optiune un output:\n",
    "\n",
    "```bash\n",
    "Batch: 1\n",
    "-------------------------------------------\n",
    "+----+--------------------+----------+---------+------+--------------------+-------------+\n",
    "| key|               value|     topic|partition|offset|           timestamp|timestampType|\n",
    "+----+--------------------+----------+---------+------+--------------------+-------------+\n",
    "|NULL|[62 69 67 20 64 6...|bd-topic-2|        0|     0|2024-04-12 13:21:...|            0|\n",
    "+----+--------------------+----------+---------+------+--------------------+-------------+\n",
    "```\n",
    "\n",
    "- `key` : este null deoarece am folosit console producer si nu am specificat vreo cheie anume\n",
    "- `value` : este mesajul in sine, in format binar\n",
    "- `topic` : numele topicului\n",
    "- `partition` : numarul partitiei, folosind configurarile default, automat avem doar o partitie\n",
    "- `offset` : offsetul mesajului\n",
    "- `timestamp` : timestampul mesajului\n",
    "- `timestampType`: Acesta poate fi 0 sau 1, 0 reprezinta un timestamp de tip _CreateTime_ adica momentul in care a fost adugat in borker, fiind cel default, iar 1 reprezinta un timestamp de tip _LogAppendTime_ adica producerul a specificat un timestamp.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:39:31.158855Z",
     "start_time": "2024-05-08T12:39:31.150616Z"
    }
   },
   "source": [
    "query.stop()"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dupa cum am mentionat anterior, mesajul nu trebuie sa fie un simplu string, el poate fi orice standard. In cele ce urmeaza vom crea un script pentru a trimite cuvinte in Kafka in format JSON si apoi vom citi aceste cuvinte.\n",
    "Pentru aceasta vom folosi un producer in Python care va trimite cuvinte in format JSON in Kafka folosind libraria [confluent-kafka-python](https://github.com/confluentinc/confluent-kafka-python).\n",
    "Scripturile aferente se pot gasi in Notebookul `KafkaProducers.ipynb`.\n",
    "\n",
    "Pentru a instala libraria folosim in containerul de jupyter:\n",
    "\n",
    "```bash\n",
    "conda install conda-forge::python-confluent-kafka\n",
    "```\n",
    "\n",
    "Vom trimite mesaje cu key si headers pentru a ilustra mai bine atributele puse la dispozitie de Kafka.\n",
    "\n",
    "Mai intai vom crea un topic numit `bd-words` **acest topic** trebuie se fie cel din producerul de Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avand headers trebuie sa le parsam, deoarece ele sunt in format binar. Pentru a face acest lucru vom crea un UDF care va face aceasta conversie. Headersul va fi un array de structuri, fiecare structura avand doua campuri: `key` de tipul string si `value` de tipul binary.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:42:55.893155Z",
     "start_time": "2024-05-08T12:42:55.887187Z"
    }
   },
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "header_schema = T.ArrayType(T.StructType([\n",
    "    T.StructField(\"key\", T.StringType(), True),\n",
    "    T.StructField(\"value\", T.StringType(), True)\n",
    "]))\n",
    "\n",
    "\n",
    "@F.udf(returnType=header_schema)\n",
    "def parse_headers(headers):\n",
    "    if headers is None:\n",
    "        return []\n",
    "    return [{'key': header.key, 'value': header.value.decode('utf-8')} for header in headers]"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:43:07.338086Z",
     "start_time": "2024-05-08T12:43:07.334802Z"
    }
   },
   "source": [
    "KAFKA_BOOTSTRAP_SERVER = 'bd-kafka:29092'\n",
    "TOPIC_WORDS = 'bd-words'\n",
    "sourceArchiveDir = os.path.join(\n",
    "    data_path, \"checkPointDir\", \"kafka\", \"wordsJson\")"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:43:25.035847Z",
     "start_time": "2024-05-08T12:43:24.970741Z"
    }
   },
   "source": [
    "df = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVER)\n",
    "    .option(\"subscribe\", TOPIC_WORDS)\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option(\"includeHeaders\", True)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "df.printSchema()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      " |-- headers: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- key: string (nullable = true)\n",
      " |    |    |-- value: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:43:41.546710Z",
     "start_time": "2024-05-08T12:43:41.543603Z"
    }
   },
   "source": [
    "word_schema = T.StructType([\n",
    "    T.StructField(\"word\", T.StringType(), True),\n",
    "    T.StructField(\"createdAt\", T.TimestampType(), True)\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:43:43.171243Z",
     "start_time": "2024-05-08T12:43:43.048930Z"
    }
   },
   "source": [
    "parsed_df = (\n",
    "    df.selectExpr(\"CAST(value AS STRING) as stringVal\",\n",
    "                  \"CAST(key AS STRING) as keyKafka\", \"headers\")\n",
    "    .withColumn(\"parsedValue\", F.from_json(F.col(\"stringVal\"), word_schema))\n",
    "    .withColumn(\"parsedHeaders\", parse_headers(F.col(\"headers\")))\n",
    "    .select(\"keyKafka\", \"parsedValue\", F.explode(\"parsedHeaders\").alias(\"header\"))\n",
    "    .select(\"keyKafka\", \"parsedValue.*\", \"header.*\")\n",
    ")\n",
    "parsed_df.printSchema()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- keyKafka: string (nullable = true)\n",
      " |-- word: string (nullable = true)\n",
      " |-- createdAt: timestamp (nullable = true)\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:44:56.166534Z",
     "start_time": "2024-05-08T12:44:20.092727Z"
    }
   },
   "source": [
    "query = (\n",
    "    parsed_df.writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", sourceArchiveDir)\n",
    "    .option(\"truncate\", False)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[27], line 10\u001B[0m\n\u001B[1;32m      1\u001B[0m query \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m      2\u001B[0m     parsed_df\u001B[38;5;241m.\u001B[39mwriteStream\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconsole\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m      8\u001B[0m )\n\u001B[0;32m---> 10\u001B[0m query\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:221\u001B[0m, in \u001B[0;36mStreamingQuery.awaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination(\u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m))\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 221\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1314\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1036\u001B[0m connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_connection()\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1038\u001B[0m     response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1039\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m binary:\n\u001B[1;32m   1040\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection_guard(connection)\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001B[0m, in \u001B[0;36mClientServerConnection.send_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    510\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 511\u001B[0m         answer \u001B[38;5;241m=\u001B[39m smart_decode(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream\u001B[38;5;241m.\u001B[39mreadline()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m    512\u001B[0m         logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnswer received: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(answer))\n\u001B[1;32m    513\u001B[0m         \u001B[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001B[39;00m\n\u001B[1;32m    514\u001B[0m         \u001B[38;5;66;03m# answer before the socket raises an error.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/socket.py:706\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    704\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 706\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39mrecv_into(b)\n\u001B[1;32m    707\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    708\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PENTRU A VEDEA REZULATE NU UITATI SA PORNITI PRODUCERUL**\n",
    "\n",
    "Pentru un mesaj vom avea un output de forma:\n",
    "\n",
    "```bash\n",
    "+--------------------------------+------+-------------------+------------+----------------+\n",
    "|keyKafka                        |word  |createdAt          |key         |value           |\n",
    "+--------------------------------+------+-------------------+------------+----------------+\n",
    "|89dbf672029e4675abf9a378232e5044|radish|2024-04-06 10:05:00|content-type|application/json|\n",
    "+--------------------------------+------+-------------------+------------+----------------+\n",
    "```\n",
    "\n",
    "- `keyKafka` : cheia mesajului pe care am trimis-o din producer, un uuid\n",
    "- `word` : cuvantul trimis in mesaj\n",
    "- `createdAt` : timestamp-ul mesajului trimis din producer\n",
    "- `key` : cheia headerului trimis din producer, in cazul nostru _content-type_\n",
    "- `value` : valoarea headerului trimis din producer, in cazul nostru _application/json_\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:45:05.180577Z",
     "start_time": "2024-05-08T12:45:05.172640Z"
    }
   },
   "source": [
    "query.stop()"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing To Kafka\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pana acum doar am folosit Kafka ca si consumer, insa o aplicatie Spark poate sa scrie in Kafka la fel de usor. Pentru a face acest lucru vom folosi metoda `writeStream` a obiectului `DataFrame` si metoda `format` pentru a specifica formatul de scriere, in cazul nostru `kafka`. Pentru a specifica serverele de bootstrapping vom utiliza metoda `option` si cheia `kafka.bootstrap.servers`. Pentru a specifica topicul vom folosi metoda `option` si cheia `topic`. Nu vom specifica `outputMode` deoarece Kafka nu suporta `update` si `complete`, ci doar `append`. Este obligatoriu sa specificam `checkpointLocation` pentru a pastra offseturile si pentru a nu pierde datele in cazul unui esec.\n",
    "\n",
    "**OBS** Pana acum am creat topicurile folosind comanda `kafka-topics`, insa putem crea topicuri si direct din Spark, insa acest lucru nu este recomandat in productie, deoarece nu putem specifica toate configurarile necesare pentru un topic. Asadar, pentru a ne invata cu best practices vom crea un topic utilizand kafka-topics in terminal.\n",
    "\n",
    "Vom folosi codul anteriror pentru citire din Kafka si vom scrie rezultatul intr-un nou topic numit `bd-words-output`. Pentru a vedea in mod live rezultatele vom lansa un consumer in terminal.\n",
    "\n",
    "Pentru ca in consumer sa vedem datele efective, acestea trebuie sa fie in coloana `value`, asadar vom crea un nou DataFrame care va avea aceasta coloana.\n",
    "\n",
    "**NU UITATI SA PORNITI PRODUCERUL SI DUPA CREAREA TOPICULUI SA PORNITI CONSUMERUL**\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:45:47.889278Z",
     "start_time": "2024-05-08T12:45:47.886320Z"
    }
   },
   "source": [
    "OUTPUT_TOPIC = 'bd-words-output'\n",
    "sourceArchiveDirOutput = os.path.join(\n",
    "    data_path, \"checkPointDir\", \"kafka\", \"wordsJsonOutput\")"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:45:50.271582Z",
     "start_time": "2024-05-08T12:45:50.267560Z"
    }
   },
   "source": [
    "@F.udf(returnType=T.StringType())\n",
    "def create_json(keyKafka, word, createdAt):\n",
    "    return f'{{\"keyKafka\": \"{keyKafka}\", \"word\": \"{word}\", \"createdAt\": \"{createdAt}\"}}'"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:46:01.561055Z",
     "start_time": "2024-05-08T12:46:01.508057Z"
    }
   },
   "source": [
    "output_df = (\n",
    "    parsed_df\n",
    "    .withColumn(\"value\", create_json(F.col(\"keyKafka\"), F.col(\"word\"), F.col(\"createdAt\")))\n",
    "    .select(\"value\")\n",
    ")\n",
    "output_df.printSchema()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:48:57.831359Z",
     "start_time": "2024-05-08T12:46:19.472191Z"
    }
   },
   "source": [
    "query = (\n",
    "    output_df\n",
    "    .withColumnRenamed(\"key\", \"keyHeader\")\n",
    "    .writeStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVER)\n",
    "    .option(\"topic\", OUTPUT_TOPIC)\n",
    "    .option(\"checkpointLocation\", sourceArchiveDirOutput)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[32], line 12\u001B[0m\n\u001B[1;32m      1\u001B[0m query \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m      2\u001B[0m     output_df\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39mwithColumnRenamed(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkey\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkeyHeader\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m     10\u001B[0m )\n\u001B[0;32m---> 12\u001B[0m query\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:221\u001B[0m, in \u001B[0;36mStreamingQuery.awaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination(\u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m))\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 221\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1314\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1036\u001B[0m connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_connection()\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1038\u001B[0m     response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1039\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m binary:\n\u001B[1;32m   1040\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection_guard(connection)\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001B[0m, in \u001B[0;36mClientServerConnection.send_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    510\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 511\u001B[0m         answer \u001B[38;5;241m=\u001B[39m smart_decode(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream\u001B[38;5;241m.\u001B[39mreadline()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m    512\u001B[0m         logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnswer received: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(answer))\n\u001B[1;32m    513\u001B[0m         \u001B[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001B[39;00m\n\u001B[1;32m    514\u001B[0m         \u001B[38;5;66;03m# answer before the socket raises an error.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/socket.py:706\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    704\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 706\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39mrecv_into(b)\n\u001B[1;32m    707\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    708\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In consumerul din terminal vom vedea mesajele sub forma:\n",
    "\n",
    "```bash\n",
    "{\"keyKafka\": \"be76d1338be44ef995ddc1c403e37684\", \"word\": \"olive\", \"createdAt\": \"2024-04-06 10:48:00\"}\n",
    "{\"keyKafka\": \"63aa2397c6264556869978da90ee6f59\", \"word\": \"eggplant\", \"createdAt\": \"2024-04-06 10:14:00\"}\n",
    "{\"keyKafka\": \"5b03b2d54edd40dbb9dc39d832eb5500\", \"word\": \"shallot\", \"createdAt\": \"2024-04-06 10:14:00\"}\n",
    "{\"keyKafka\": \"9fc1667cde514d5b887f9e793fed6c20\", \"word\": \"banana\", \"createdAt\": \"2024-04-06 10:16:00\"}\n",
    "{\"keyKafka\": \"38abe98e727a42faa3b20e66d1ae4736\", \"word\": \"mulberry\", \"createdAt\": \"2024-04-06 10:40:00\"}\n",
    "{\"keyKafka\": \"1ba61be15e134133bb96f460f5611ea0\", \"word\": \"cherry\", \"createdAt\": \"2024-04-06 10:48:00\"}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:49:01.246391Z",
     "start_time": "2024-05-08T12:49:01.238609Z"
    }
   },
   "source": [
    "query.stop()"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:49:04.997834Z",
     "start_time": "2024-05-08T12:49:04.260094Z"
    }
   },
   "source": [
    "spark.stop()"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing To Multiple Sinks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ForeachBatch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operatia `foreachBatch` ne permite sa utilizam logica customizata la scrierea datelor pentru fiecare _micro-batch_. Aceasta operatie ne permite sa executam o functie pentru fiecare _micro-batch_ in parte si primeste doi paremetri: batchul in sine si id-ul batchului/epocii. Este utila in special pentru a scrie in surse care nu au implementatat in mod nativ scrierea pentru streaming. In cazul nostru vom folosi aceasta operatie pentru a scrie in Kafka si intr-o baza de date SQL, anume Postgres. Postgres nu dispune in mod nativ de un format de streaming. (Daca suntem familiari cu conceptul de R2DBC din Spring vom putea zice ca scriem datele in mod _reactiv_ .)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup The Environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pentru a ne putea conecta la Postgres ne trebuie un jar specific. In cazul nostru vom folosi `org.postgresql:postgresql:42.7.3` care poate fi gasit in directorul jars. Acesta contine toate dependentele necesare pentru a realiza conexiunea intre Spark si Postgres. Unde:\n",
    "\n",
    "- `org.postgresql` : este grupul care a creat jarul\n",
    "- `postgresql` : este numele artefactului\n",
    "- `42.7.3` : este versiunea jarului si a Postgres-ului\n",
    "\n",
    "Vom pune acest jar in volumul containerului de jupyter la calea `/home/jovyan/.ivy2/jars`. **ATENTIE IN ACEL FOLDER SUNT SI ALTE JARURI FOLOSITE DE SPARK, ASADAR NU MODIFICATI ALTCEVA DECAT SA ADAUGATI JARUL NOU.**\n",
    "\n",
    "Dupa ce adaugam jarul, va trebui sa recream sesiunesa Spark. Daca dupa recreare intampinati probleme, reproniti containerul si dupa creati noua sesiune.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:59:53.301871Z",
     "start_time": "2024-05-08T12:59:45.382864Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Writing To Multiple Sources\")\n",
    "    # kafka\n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0')\n",
    "    # jarul nou introdus, atentie la calea data\n",
    "    .config('spark.jars', '/home/jovyan/.ivy2/jars/postgresql-42.7.3.jar')\n",
    "    .config(\"spark.sql.shuffle.partitions\", 16)\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T12:59:57.203406Z",
     "start_time": "2024-05-08T12:59:55.120823Z"
    }
   },
   "source": [
    "# recrearea codului anterior pentru cuvintele din kafka\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import os\n",
    "\n",
    "data_path = os.path.join(os.getcwd(), \"referat\")\n",
    "\n",
    "KAFKA_BOOTSTRAP_SERVER = 'bd-kafka:29092'\n",
    "TOPIC_WORDS = 'bd-words'\n",
    "OUTPUT_TOPIC = 'bd-words-output'\n",
    "sourceArchiveDirOutput = os.path.join(\n",
    "    data_path, \"checkPointDir\", \"kafka\", \"wordsJsonOutput\")\n",
    "\n",
    "word_schema = T.StructType([\n",
    "    T.StructField(\"word\", T.StringType(), True),\n",
    "    T.StructField(\"createdAt\", T.TimestampType(), True)\n",
    "])\n",
    "\n",
    "header_schema = T.ArrayType(T.StructType([\n",
    "    T.StructField(\"key\", T.StringType(), True),\n",
    "    T.StructField(\"value\", T.StringType(), True)\n",
    "]))\n",
    "\n",
    "\n",
    "@F.udf(returnType=header_schema)\n",
    "def parse_headers(headers):\n",
    "    if headers is None:\n",
    "        return []\n",
    "    return [{'key': header.key, 'value': header.value.decode('utf-8')} for header in headers]\n",
    "\n",
    "\n",
    "@F.udf(returnType=T.StringType())\n",
    "def create_json(keyKafka, word, createdAt):\n",
    "    return f'{{\"keyKafka\": \"{keyKafka}\", \"word\": \"{word}\", \"createdAt\": \"{createdAt}\"}}'\n",
    "\n",
    "\n",
    "parsed_df = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVER)\n",
    "    .option(\"subscribe\", TOPIC_WORDS)\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option(\"includeHeaders\", True)\n",
    "    .load()\n",
    "    .selectExpr(\"CAST(value AS STRING) as stringVal\",\n",
    "                \"CAST(key AS STRING) as keyKafka\", \"headers\")\n",
    "    .withColumn(\"parsedValue\", F.from_json(F.col(\"stringVal\"), word_schema))\n",
    "    .withColumn(\"parsedHeaders\", parse_headers(F.col(\"headers\")))\n",
    "    .select(\"keyKafka\", \"parsedValue\", F.explode(\"parsedHeaders\").alias(\"header\"))\n",
    "    .select(\"keyKafka\", \"parsedValue.*\", \"header.*\")\n",
    ")\n",
    "\n",
    "parsed_df.printSchema()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- keyKafka: string (nullable = true)\n",
      " |-- word: string (nullable = true)\n",
      " |-- createdAt: timestamp (nullable = true)\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inainte de a continua cu codul din Pyspark sa cream o baza de date si un tabel in aceasta pentru a salva informatia. Structura tabelului va fi:\n",
    "\n",
    "| Column    | Type         |\n",
    "| --------- | ------------ |\n",
    "| keyKafka  | VARCHAR(255) |\n",
    "| word      | Text         |\n",
    "| createdAt | TIMESTAMP    |\n",
    "| keyHeader | VARCHAR(255) |\n",
    "| value     | Text         |\n",
    "\n",
    "Pentru a deschide **PgAdmin** vom merge la adresa `localhost:5050` si ne vom loga cu userul specificat in container, anume:\n",
    "\n",
    "- `email` : admin@admin.com\n",
    "- `password` : admin\n",
    "\n",
    "Vom adauga un nou server cu urmatoarele configurari:\n",
    "\n",
    "- `General` :\n",
    "  - `Name` : referat\n",
    "- `Connection` : \n",
    "  - `Host name/address` : bd-postgres \n",
    "  - `Port` : 5432 - `Maintenance database` : postgres \n",
    "  - `Username` : myuser \n",
    "  - `Password` : mypassword\n",
    "  Restul setarilor le lasam default.\n",
    "\n",
    "Dupa ce ne conectam la server, daca apasam pe `referat` vom vedea `databases`. Click dreapta pe `databases` -> `Create` -> `Database` si vom crea o baza de date cu numele `mydb`. Dupa ce am creat baza de date, click dreapta pe `mydb` -> `Query Tool` si vom crea tabelul cu urmatorul cod SQL:\n",
    "\n",
    "```sql\n",
    "CREATE TABLE word_table (\n",
    "    keyKafka VARCHAR(255),\n",
    "    word TEXT,\n",
    "    createdAt TIMESTAMP,\n",
    "    keyHeader VARCHAR(255),\n",
    "    value TEXT\n",
    ");\n",
    "```\n",
    "\n",
    "In acest moment avem setata baza de date pentru a scrie.\n",
    "\n",
    "In continuare vom crea functia pentru _forEachBatch_ pentru a scrie atat in Kafka cat si in Postgres.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T13:00:02.055235Z",
     "start_time": "2024-05-08T13:00:02.048314Z"
    }
   },
   "source": [
    "def write_to_kafka_and_postgres(batch_df, batch_id):\n",
    "    print(f\"Writing batch {batch_id}\")\n",
    "    batch_df.persist()\n",
    "    try:\n",
    "        # batch_df.show() # putem apela show\n",
    "        batch_df.withColumn(\"value\", create_json(F.col(\"keyKafka\"), F.col(\"word\"), F.col(\"createdAt\"))) \\\n",
    "            .select(\"value\") \\\n",
    "            .write.format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVER) \\\n",
    "            .option(\"topic\", OUTPUT_TOPIC) \\\n",
    "            .save()\n",
    "        print(\"Datele introduse in Kafka.\")\n",
    "\n",
    "        (batch_df.withColumnRenamed(\"key\", \"keyHeader\")\n",
    "            .write.format(\"jdbc\")\n",
    "            .option(\"url\", \"jdbc:postgresql://bd-postgres:5432/mydb\")\n",
    "            .option(\"dbtable\", \"word_table\")\n",
    "            .option(\"user\", \"myuser\")\n",
    "            .option(\"password\", \"mypassword\")\n",
    "            .option('driver', 'org.postgresql.Driver')\n",
    "            .mode(\"append\")\n",
    "            .save())\n",
    "        print(\"Datele introduse in PostgreSQL.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        batch_df.unpersist()"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De observat este ca nu am folosit `writeStream` ci `write`deoarece functia `forEachBatch` va apela callback-ul pentru fiecare _micro-batch_ in parte si nu pentru intregul stream.\n",
    "Am folosit `batch_df.persist()` pentru a pastra datele in memorie si a le folosi in ambele operatii, iar apoi am folosit `batch_df.unpersist()` pentru a elibera memoria.\n",
    "\n",
    "Pentru a scrie in Postgres am folosit `write.format(\"jdbc\")` si am specificat urmatoarele:\n",
    "\n",
    "- `url` : adresa bazei de date, atentie `mydb` este baza de date pe care am creat-o anterior\n",
    "- `dbtable` : numele tabelului in care vom scrie\n",
    "- `user` : userul pentru baza de date\n",
    "- `password` : parola pentru baza de date\n",
    "- `driver` : driverul pentru Postgres\n",
    "- `mode` : modul in care vom scrie, in cazul nostru `append` pentru a adauga datele la cele existente\n",
    "\n",
    "Pentru a introduce mesaje in topicul de citire din Kafka vom rula producerul pentru cuvinte. Pentru a vedea rezultatele in Kafka vom folosi un console consumer, iar pentru a vedea rezultatele in Postgres vom folosi PgAdmin.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T13:00:50.483617Z",
     "start_time": "2024-05-08T13:00:06.207674Z"
    }
   },
   "source": [
    "query = parsed_df.writeStream \\\n",
    "    .foreachBatch(write_to_kafka_and_postgres) \\\n",
    "    .option(\"checkpointLocation\", sourceArchiveDirOutput) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing batch 77\n",
      "Datele introduse in Kafka.\n",
      "Datele introduse in PostgreSQL.\n",
      "Writing batch 78\n",
      "Datele introduse in Kafka.\n",
      "Datele introduse in PostgreSQL.\n",
      "Writing batch 79\n",
      "Datele introduse in Kafka.\n",
      "Datele introduse in PostgreSQL.\n",
      "Writing batch 80\n",
      "Datele introduse in Kafka.\n",
      "Datele introduse in PostgreSQL.\n",
      "Writing batch 81\n",
      "Datele introduse in Kafka.\n",
      "Datele introduse in PostgreSQL.\n",
      "Writing batch 82\n",
      "Datele introduse in Kafka.\n",
      "Datele introduse in PostgreSQL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 6\u001B[0m\n\u001B[1;32m      1\u001B[0m query \u001B[38;5;241m=\u001B[39m parsed_df\u001B[38;5;241m.\u001B[39mwriteStream \\\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;241m.\u001B[39mforeachBatch(write_to_kafka_and_postgres) \\\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcheckpointLocation\u001B[39m\u001B[38;5;124m\"\u001B[39m, sourceArchiveDirOutput) \\\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n\u001B[0;32m----> 6\u001B[0m query\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:221\u001B[0m, in \u001B[0;36mStreamingQuery.awaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination(\u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m))\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 221\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1314\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1036\u001B[0m connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_connection()\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1038\u001B[0m     response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1039\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m binary:\n\u001B[1;32m   1040\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection_guard(connection)\n",
      "File \u001B[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001B[0m, in \u001B[0;36mClientServerConnection.send_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    510\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 511\u001B[0m         answer \u001B[38;5;241m=\u001B[39m smart_decode(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream\u001B[38;5;241m.\u001B[39mreadline()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m    512\u001B[0m         logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnswer received: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(answer))\n\u001B[1;32m    513\u001B[0m         \u001B[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001B[39;00m\n\u001B[1;32m    514\u001B[0m         \u001B[38;5;66;03m# answer before the socket raises an error.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.11/socket.py:706\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    704\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 706\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39mrecv_into(b)\n\u001B[1;32m    707\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    708\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daca suntem conectati la topicul de output, _bd-words-output_, vom vedea mesajele cum apar in timp real, iar daca facem query pe tabelul `word_table` din PgAdmin vom vedea cum datele au fost introduse in baza de date.\n",
    "\n",
    "De exemplu, streamul a parcurs 5 date. In Kafka vom avea:\n",
    "\n",
    "![Kafka BD Sink](./images/kafka/kafkaBdSink.png)\n",
    "\n",
    "Iar in Postgres , ruland\n",
    "\n",
    "```sql\n",
    "SELECT * FROM word_table;\n",
    "```\n",
    "\n",
    "vom avea:\n",
    "\n",
    "![Postgres Output Words](./images/postgres/postgresOutputWords.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T13:01:20.168517Z",
     "start_time": "2024-05-08T13:01:20.147318Z"
    }
   },
   "source": [
    "query.stop()"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliografie\n",
    "\n",
    "- Partea introductiva\n",
    "  - https://subhamkharwal.medium.com/pyspark-structured-streaming-read-from-files-c46fa0ce8888\n",
    "  - https://www.projectpro.io/recipes/perform-window-operations-during-spark-structured-streaming\n",
    "  - https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html\n",
    "- Partea de shuffle\n",
    "  - https://spark.apache.org/docs/latest/sql-performance-tuning.html\n",
    "- Partea de Kafka\n",
    "  - https://subhamkharwal.medium.com/pyspark-structured-streaming-read-from-kafka-64c40767155f\n",
    "  - https://www.javatpoint.com/apache-kafka\n",
    "  - https://docs.confluent.io/kafka/introduction.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
